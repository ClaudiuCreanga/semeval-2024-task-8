{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune BERT base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmarchitan/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer, BertModel, BatchEncoding\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from lib.utils import get_current_date\n",
    "from lib.utils.constants import Subtask, Track, PreprocessTextLevel, PoolingStrategy, DatasetType\n",
    "from lib.utils.models import sequential_fully_connected\n",
    "from lib.data.loading import load_train_dev_test_df, build_data_loader\n",
    "from lib.data.tokenizer import get_tokenizer\n",
    "from lib.models import get_model\n",
    "from lib.training.loss import get_loss_fn\n",
    "from lib.training.metric import get_metric\n",
    "from lib.training.loops import training_loop, make_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = os.path.relpath(\"../config.json\")\n",
    "\n",
    "CONFIG = {}\n",
    "with open(CONFIG_FILE) as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "Train/dev split... (df_train.shape: (119757, 5))\n",
      "Loading test data...\n",
      "Cleaning texts with preprocess level `PreprocessTextLevel.LIGHT`...\n",
      "df_train.shape: (95805, 5)\n",
      "df_dev.shape: (23952, 5)\n",
      "df_test.shape: (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_dev, df_test = load_train_dev_test_df(\n",
    "    task=Subtask(CONFIG[\"task\"]),\n",
    "    track=Track(CONFIG[\"track\"]),\n",
    "    data_dir=\"../data/original_data\",\n",
    "    label_column=CONFIG[\"data\"][\"label_column\"],\n",
    "    test_size=CONFIG[\"data\"][\"test_size\"],\n",
    "    preprocess_text_level=PreprocessTextLevel(\n",
    "        CONFIG[\"data\"][\"preprocess_text_level\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "print(f\"df_dev.shape: {df_dev.shape}\")\n",
    "print(f\"df_test.shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = CONFIG[\"data\"][\"max_len\"]\n",
    "tokenizer = get_tokenizer(**CONFIG[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which layer should we use for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pooled ouput of the [CLS] token\n",
    "* One single hidden layer 0-max_number_of_layers\n",
    "* First 4 layers + concat\n",
    "* First 4 layers + mean\n",
    "* First 4 layers + max\n",
    "* Last 4 layers + concat\n",
    "* Last 4 layers + mean\n",
    "* Last 4 layers + max\n",
    "* All layers + concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean cuda memroy\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = build_data_loader(\n",
    "    df_train[:10],\n",
    "    tokenizer,\n",
    "    max_len=CONFIG[\"data\"][\"max_len\"],\n",
    "    batch_size=CONFIG[\"data\"][\"batch_size\"],\n",
    "    label_column=CONFIG[\"data\"][\"label_column\"],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained(\n",
    "    CONFIG[\"model_config\"][\"pretrained_model_name\"],\n",
    "    return_dict=False,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "bert_num_layers = len(bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in bert.named_parameters():\n",
    "    print(p[0])\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTWithLayerSelection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name: str,\n",
    "        out_size: int = 1,\n",
    "        dropout_p: float = 0.5,\n",
    "        selected_layers: [int] = [-1],\n",
    "        fc: [int] = [],\n",
    "        out_activation: str | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.selected_layers = selected_layers\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            pretrained_model_name, return_dict=False, output_hidden_states=True\n",
    "        )\n",
    "        # self.drop_bert = nn.Dropout(dropout_p)\n",
    "\n",
    "        input_size = len(selected_layers) * self.bert.config.hidden_size\n",
    "        self.out = sequential_fully_connected(input_size, out_size, fc, dropout_p)\n",
    "\n",
    "        self.out_activation = None\n",
    "        if out_activation == \"sigmoid\":\n",
    "            self.out_activation = nn.Sigmoid()\n",
    "\n",
    "        self.freeze_transformer_layer()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids, attention_mask)\n",
    "        hidden_states = bert_outputs[2]\n",
    "        bert_cls_features = torch.cat(\n",
    "            [hidden_states[i][:, 0, :] for i in self.selected_layers],\n",
    "            dim=1,\n",
    "        )\n",
    "        # output = self.drop_bert(pooled_output)\n",
    "        output = self.out(bert_cls_features)\n",
    "\n",
    "        if self.out_activation is not None:\n",
    "            output = self.out_activation(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def freeze_transformer_layer(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_transformer_layer(self):\n",
    "        # BERT used only for feature extraction\n",
    "        pass\n",
    "\n",
    "    def get_predictions_from_outputs(self, outputs):\n",
    "        if self.out_activation is None:\n",
    "            return outputs.flatten().tolist()\n",
    "        else:\n",
    "            return torch.round(outputs).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layers_to_use = [-1, -2, -3, -4]\n",
    "CONFIG[\"model_config\"][\"selected_layers\"] = bert_layers_to_use\n",
    "bert_with_layer_selection = BERTWithLayerSelection(**CONFIG[\"model_config\"]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "true = [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "predictions = [1.0, 1.0]\n",
      "true = [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Bert with layer selection\n",
    "\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    ids = batch[\"id\"]\n",
    "    input_ids = batch[\"input_ids\"]  # .to(device)\n",
    "    attention_mask = batch[\"attention_mask\"]  # .to(device)\n",
    "    targets = batch[\"target\"].to(DEVICE)\n",
    "\n",
    "    outputs = bert_with_layer_selection(\n",
    "        input_ids=input_ids, attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    predictions = bert_with_layer_selection.get_predictions_from_outputs(outputs)\n",
    "    true = targets.flatten().tolist()\n",
    "\n",
    "    print(f\"predictions = {predictions}\")\n",
    "    print(f\"true = {true}\")\n",
    "\n",
    "    # expected_input_size = len(bert_layers_to_use) * bert.config.hidden_size\n",
    "    # print(f\"expected_input_size: {expected_input_size}\")\n",
    "\n",
    "    # embs = bert(input_ids, attention_mask)\n",
    "    # print(f\"embs[0].shape: {embs[0].shape}\")\n",
    "    # print(f\"embs[1].shape: {embs[1].shape}\")\n",
    "\n",
    "    # hidden_states = embs[2]\n",
    "    # selected_layers = [hidden_states[i] for i in bert_layers_to_use]\n",
    "\n",
    "    # # selected_layers = torch.cat(selected_layers, dim=2)\n",
    "    # # print(f\"selected_layers.shape: {selected_layers.shape}\")\n",
    "\n",
    "    # # mean_layer = torch.mean(selected_layers, dim=1)\n",
    "    # # print(f\"mean_layer.shape: {mean_layer.shape}\")\n",
    "\n",
    "    # cls_features = [layer[:, 0, :] for layer in selected_layers]\n",
    "    # cls_features = torch.cat(cls_features, dim=1)\n",
    "    # print(f\"cls_features.shape: {cls_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Layer selection: first_4_layers #####\n",
      "\n",
      "Will save results to: ../runs/28-11-2023_09:34:44-SubtaskA-monolingual-bert_with_layer_selection_first_4_layers\n",
      "\n",
      "Epoch 1/1\n",
      "Freeze transformeer\n",
      "--------------------\n",
      "Batch=[1/13]; Loss=[0.64347]; Acc. Metric=0.875\n",
      "Train Loss: 0.69481; Train Metric: 0.53000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.68766; Validation Metric: 0.58000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "##### Layer selection: last_4_layers #####\n",
      "\n",
      "Will save results to: ../runs/28-11-2023_09:34:57-SubtaskA-monolingual-bert_with_layer_selection_last_4_layers\n",
      "\n",
      "Epoch 1/1\n",
      "Freeze transformeer\n",
      "--------------------\n",
      "Batch=[1/13]; Loss=[0.72704]; Acc. Metric=0.375\n",
      "Train Loss: 0.71874; Train Metric: 0.42000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.70619; Validation Metric: 0.44000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "selected_layers_options = {\n",
    "    # \"1-1_layers\": [1],\n",
    "    # \"1-2_layers\": [1, 2],\n",
    "    # \"1-3_layers\": [1, 2, 3],\n",
    "    # \"1-4_layers\": [1, 2, 3, 4],\n",
    "    # \"1-5_layers\": [1, 2, 3, 4, 5],\n",
    "    # \"1-6_layers\": [1, 2, 3, 4, 5, 6],\n",
    "    # \"1-7_layers\": [1, 2, 3, 4, 5, 6, 7],\n",
    "    # \"1-8_layers\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    # \"1-9_layers\": [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    # \"1-10_layers\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    # \"1-11_layers\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "    # \"1-12_layers\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    \"first_4_layers\": [1, 2, 3, 4],\n",
    "    \"last_4_layers\": [-1, -2, -3, -4],\n",
    "}\n",
    "\n",
    "for selection_name, selected_layers in selected_layers_options.items():\n",
    "    print(f\"##### Layer selection: {selection_name} #####\\n\")\n",
    "\n",
    "    CONFIG[\"model_config\"][\"selected_layers\"] = selected_layers\n",
    "\n",
    "    if CONFIG[\"track\"] is None:\n",
    "        results_path = (\n",
    "            f\"../runs/{get_current_date()}-{CONFIG['task']}-{CONFIG['model']}_{selection_name}\"\n",
    "        )\n",
    "    else:\n",
    "        results_path = (\n",
    "            f\"../runs/{get_current_date()}-\"\n",
    "            f\"{CONFIG['task']}-{CONFIG['track']}-{CONFIG['model']}_{selection_name}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Will save results to: {results_path}\\n\")\n",
    "    os.mkdir(results_path)\n",
    "\n",
    "    with open(results_path + \"/config.json\", \"w\") as f:\n",
    "        json.dump(CONFIG, f, indent=4)\n",
    "\n",
    "    train_dataloader = build_data_loader(\n",
    "        df_train[:100],\n",
    "        tokenizer,\n",
    "        max_len=CONFIG[\"data\"][\"max_len\"],\n",
    "        batch_size=CONFIG[\"data\"][\"batch_size\"],\n",
    "        label_column=CONFIG[\"data\"][\"label_column\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    dev_dataloader = build_data_loader(\n",
    "        df_dev[:100],\n",
    "        tokenizer,\n",
    "        max_len=CONFIG[\"data\"][\"max_len\"],\n",
    "        batch_size=CONFIG[\"data\"][\"batch_size\"],\n",
    "        label_column=CONFIG[\"data\"][\"label_column\"],\n",
    "    )\n",
    "    test_dataloader = build_data_loader(\n",
    "        df_test[:100],\n",
    "        tokenizer,\n",
    "        max_len=CONFIG[\"data\"][\"max_len\"],\n",
    "        batch_size=CONFIG[\"data\"][\"batch_size\"],\n",
    "        label_column=CONFIG[\"data\"][\"label_column\"],\n",
    "        has_targets=False if CONFIG[\"data\"][\"test_size\"] is None else True,\n",
    "    )\n",
    "\n",
    "    num_epochs = CONFIG[\"training\"][\"num_epochs\"]\n",
    "    model = get_model(CONFIG[\"model\"], CONFIG[\"model_config\"]).to(DEVICE)\n",
    "    loss_fn = get_loss_fn(CONFIG[\"training\"][\"loss\"], DEVICE)\n",
    "    optimizer_config = CONFIG[\"training\"][\"optimizer\"]\n",
    "    scheduler_config = CONFIG[\"training\"][\"scheduler\"]\n",
    "    metric_fn, is_better_metric_fn = get_metric(CONFIG[\"training\"][\"metric\"])\n",
    "    num_epochs_before_finetune = CONFIG[\"training\"][\"num_epochs_before_finetune\"]\n",
    "\n",
    "    best_model = training_loop(\n",
    "        model,\n",
    "        num_epochs,\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        loss_fn,\n",
    "        optimizer_config,\n",
    "        scheduler_config,\n",
    "        DEVICE,\n",
    "        metric_fn,\n",
    "        is_better_metric_fn,\n",
    "        results_path,\n",
    "        num_epochs_before_finetune,\n",
    "    )\n",
    "\n",
    "    make_predictions(\n",
    "        best_model,\n",
    "        test_dataloader,\n",
    "        DEVICE,\n",
    "        results_path,\n",
    "        label_column=CONFIG[\"data\"][\"label_column\"],\n",
    "        file_format=CONFIG[\"submission_format\"],\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation\n",
      "Accuracy: 58.00%\n",
      "/Users/tmarchitan/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Precision: 0.00%\n",
      "Recall: 0.00%\n",
      "F1: 0.00%\n",
      "--------------------\n",
      "Results on test\n",
      "Accuracy: 0.00%\n",
      "/Users/tmarchitan/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Precision: 0.00%\n",
      "Recall: 0.00%\n",
      "F1: 0.00%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../scores_and_plots.py --results-dir \"../runs/28-11-2023_09:34:44-SubtaskA-monolingual-bert_with_layer_selection_first_4_layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation\n",
      "Accuracy: 44.00%\n",
      "Precision: 37.50%\n",
      "Recall: 50.00%\n",
      "F1: 42.86%\n",
      "--------------------\n",
      "Results on test\n",
      "Accuracy: 76.00%\n",
      "Precision: 100.00%\n",
      "Recall: 76.00%\n",
      "F1: 86.36%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../scores_and_plots.py --results-dir \"../runs/28-11-2023_09:34:57-SubtaskA-monolingual-bert_with_layer_selection_last_4_layers\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
