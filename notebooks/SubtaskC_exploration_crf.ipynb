{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmarchitan/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from lib.utils import get_device\n",
    "from lib.utils.constants import Subtask, Track, PreprocessTextLevel, DatasetType\n",
    "from lib.data.loading import load_train_dev_test_df\n",
    "from lib.data.tokenizer import get_tokenizer\n",
    "from lib.training.optimizer import get_optimizer, get_scheduler\n",
    "from lib.training.loss import get_loss_fn\n",
    "from lib.training.metric import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Will save results to: ../runs/SubtaskC\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE_PATH = os.path.relpath(\"../config.json\")\n",
    "\n",
    "config = {}\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "results_dir = os.path.relpath(\"../runs/SubtaskC\")\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "print(f\"Will save results to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'SubtaskC',\n",
       " 'submission_format': 'csv',\n",
       " 'model': 'longformer_crf',\n",
       " 'tokenizer': {'model_name': 'longformer',\n",
       "  'pretrained_name': 'allenai/longformer-base-4096'},\n",
       " 'data': {'dataset_type': 'longformer_dataset',\n",
       "  'dataset_type_settings': {},\n",
       "  'data_dir': './data/original_data',\n",
       "  'label_column': 'label',\n",
       "  'max_len': 1024,\n",
       "  'batch_size': 16,\n",
       "  'test_size': 0.2,\n",
       "  'preprocess_text_level': 0},\n",
       " 'model_config': {'pretrained_model_name': 'allenai/longformer-base-4096',\n",
       "  'out_size': 2,\n",
       "  'dropout_p': 0.2},\n",
       " 'training': {'num_epochs': 5,\n",
       "  'num_epochs_before_finetune': 0,\n",
       "  'optimizer': {'AdamW': {'freeze_lr': 0.001, 'finetune_lr': 2e-05}},\n",
       "  'scheduler': {'num_warmup_steps': 50},\n",
       "  'early_stopping': {'patience': 1, 'delta': 0.001},\n",
       "  'loss': 'cross_entropy',\n",
       "  'metric': 'mae'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Track not specified in config for subtask: Subtask.SubtaskC\n",
      "Loading train data...\n",
      "Train/dev split... (df_train.shape: (3649, 3))\n",
      "Loading test data....././data/original_data/SubtaskC/SubtaskC_dev.jsonl\n",
      "df_train.shape: (2919, 3)\n",
      "df_dev.shape: (730, 3)\n",
      "df_test.shape: (505, 3)\n"
     ]
    }
   ],
   "source": [
    "task = None\n",
    "if \"task\" in config:\n",
    "    task = Subtask(config[\"task\"])\n",
    "else:\n",
    "    raise ValueError(\"Task not specified in config\")\n",
    "\n",
    "track = None\n",
    "if \"track\" in config:\n",
    "    track = Track(config[\"track\"])\n",
    "else:\n",
    "    print(f\"Warning: Track not specified in config for subtask: {task}\")\n",
    "\n",
    "dataset_type = DatasetType.TransformerTruncationDataset\n",
    "if \"dataset_type\" in config[\"data\"]:\n",
    "    dataset_type = DatasetType(config[\"data\"][\"dataset_type\"])\n",
    "\n",
    "dataset_type_settings = None\n",
    "if \"dataset_type_settings\" in config[\"data\"]:\n",
    "    dataset_type_settings = config[\"data\"][\"dataset_type_settings\"]\n",
    "\n",
    "df_train, df_dev, df_test = load_train_dev_test_df(\n",
    "    task=task,\n",
    "    track=track,\n",
    "    data_dir=f\"../{config['data']['data_dir']}\",\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    test_size=config[\"data\"][\"test_size\"],\n",
    "    preprocess_text_level=PreprocessTextLevel(\n",
    "        config[\"data\"][\"preprocess_text_level\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "print(f\"df_dev.shape: {df_dev.shape}\")\n",
    "print(f\"df_test.shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ids: np.ndarray,\n",
    "        texts: np.ndarray,\n",
    "        targets: np.ndarray | None,\n",
    "        tokenizer: PreTrainedTokenizer | None,\n",
    "        max_len: int,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer cannot be None\")\n",
    "\n",
    "        self.ids = ids\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item_id = self.ids[index]\n",
    "        text = self.texts[index]\n",
    "        target = -1 if self.targets is None else self.targets[index]\n",
    "        targets_available = False if target == -1 else True\n",
    "\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = text.replace(\"\\r\", \" \")\n",
    "\n",
    "        words = [w for w in text.split(\" \") if w != \"\"]\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Words: {words}\")\n",
    "            print(f\"Machine text start position: {target}\")\n",
    "            print()\n",
    "\n",
    "        targets = []\n",
    "        corresponding_word = []\n",
    "        tokens = []\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "\n",
    "        for idx, word in enumerate(words):\n",
    "            word_encoded = self.tokenizer.tokenize(word)  # No [CLS] or [SEP]\n",
    "            sub_words = len(word_encoded)\n",
    "\n",
    "            if targets_available:\n",
    "                is_machine_text = 1 if idx >= target else 0\n",
    "                targets.extend([is_machine_text] * sub_words)\n",
    "\n",
    "            corresponding_word.extend([idx] * sub_words)\n",
    "            tokens.extend(word_encoded)\n",
    "            input_ids.extend(self.tokenizer.convert_tokens_to_ids(word_encoded))\n",
    "            attention_mask.extend([1] * sub_words)\n",
    "\n",
    "            if self.debug:\n",
    "                print(\n",
    "                    f\"word[{idx}]:\\n\"\n",
    "                    f\"{'':-<5}> tokens: {word_encoded} (no. of subwords: {sub_words})\\n\"\n",
    "                    f\"{'':-<5}> corresponding_word: {corresponding_word[-sub_words:]}\\n\"\n",
    "                    f\"{'':-<5}> input_ids: {input_ids[-sub_words:]}\\n\"\n",
    "                    f\"{'':-<5}> is_machine_text: {is_machine_text}\"\n",
    "                )\n",
    "\n",
    "        if self.debug:\n",
    "            print()\n",
    "\n",
    "            print(f\"corresponding_word: {corresponding_word}\")\n",
    "            print(f\"tokens: {tokens}\")\n",
    "            print(f\"input_ids: {input_ids}\")\n",
    "            print(f\"attention_mask: {attention_mask}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "            print(f\"Machine text start word: {words[corresponding_word[targets.index(1)]]}\")\n",
    "            print(f\"True machine text start word: {words[target]}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "        if len(input_ids) < self.max_len - 2:\n",
    "            if targets_available:\n",
    "                targets = (\n",
    "                    [-100]\n",
    "                    + targets\n",
    "                    + [-100] * (self.max_len - len(input_ids) - 1)\n",
    "                )\n",
    "\n",
    "            corresponding_word = (\n",
    "                [-100]\n",
    "                + corresponding_word\n",
    "                + [-100] * (self.max_len - len(input_ids) - 1)\n",
    "            )\n",
    "            tokens = (\n",
    "                [self.tokenizer.bos_token]\n",
    "                + tokens\n",
    "                + [self.tokenizer.eos_token]\n",
    "                + [self.tokenizer.pad_token] * (self.max_len - len(tokens) - 2)\n",
    "            )\n",
    "            input_ids = (\n",
    "                [self.tokenizer.bos_token_id]\n",
    "                + input_ids\n",
    "                + [self.tokenizer.eos_token_id]\n",
    "                + [self.tokenizer.pad_token_id] * (self.max_len - len(input_ids) - 2)\n",
    "            )\n",
    "            attention_mask = (\n",
    "                [1]\n",
    "                + attention_mask\n",
    "                + [1]\n",
    "                + [0] * (self.max_len - len(attention_mask) - 2)\n",
    "            )\n",
    "        else:\n",
    "            if targets_available:\n",
    "                targets = [-100] + targets[: self.max_len - 2] + [-100]\n",
    "\n",
    "            corresponding_word = (\n",
    "                [-100]\n",
    "                + corresponding_word[: self.max_len - 2]\n",
    "                + [-100]\n",
    "            )\n",
    "            tokens = (\n",
    "                [self.tokenizer.bos_token]\n",
    "                + tokens[: self.max_len - 2]\n",
    "                + [self.tokenizer.eos_token]\n",
    "            )\n",
    "            input_ids = (\n",
    "                [self.tokenizer.bos_token_id]\n",
    "                + input_ids[: self.max_len - 2]\n",
    "                + [self.tokenizer.eos_token_id]\n",
    "            )\n",
    "            attention_mask = (\n",
    "                [1]\n",
    "                + attention_mask[: self.max_len - 2]\n",
    "                + [1]\n",
    "            )\n",
    "\n",
    "        encoded = {}\n",
    "        encoded[\"id\"] = item_id\n",
    "        encoded[\"text\"] = text\n",
    "        encoded[\"true_target\"] = torch.tensor(target)\n",
    "        encoded[\"corresponding_word\"] = torch.tensor(corresponding_word)\n",
    "        encoded[\"input_ids\"] = torch.tensor(input_ids)\n",
    "        encoded[\"attention_mask\"] = torch.tensor(attention_mask)\n",
    "        if targets_available:\n",
    "            encoded[\"target\"] = torch.tensor(targets)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Tokenized human position: {targets.index(1)}\")\n",
    "            print(f\"Original human position: {target}\")\n",
    "            print(f\"Full human text: {text}\\n\\n\")\n",
    "            print(f\"Human truncated text: {[w for w in text.split(' ')[:target] if w != '']}\\n\\n\")\n",
    "\n",
    "            encoded[\"partial_human_review\"] = \" \".join(\n",
    "                [w for w in text.split(' ')[:target] if w != '']\n",
    "            )\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = get_tokenizer(**config[\"tokenizer\"])\n",
    "\n",
    "train_dataset = TokenClassificationDataset(\n",
    "    ids=df_train[\"id\"].values,  # [:10],\n",
    "    texts=df_train[\"text\"].values,  # [:10],\n",
    "    targets=df_train[\"label\"].values,  # [:10],\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config[\"data\"][\"max_len\"],\n",
    "    debug=False,\n",
    ")\n",
    "dev_dataset = TokenClassificationDataset(\n",
    "    ids=df_dev[\"id\"].values,  # [:10],\n",
    "    texts=df_dev[\"text\"].values,  # [:10],\n",
    "    targets=df_dev[\"label\"].values,  # [:10],\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config[\"data\"][\"max_len\"],\n",
    "    debug=False,\n",
    ")\n",
    "test_dataset = TokenClassificationDataset(\n",
    "    ids=df_test[\"id\"].values,  # [:10],\n",
    "    texts=df_test[\"text\"].values,  # [:10],\n",
    "    targets=df_test[\"label\"].values,  # [:10],\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config[\"data\"][\"max_len\"],\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     print(f\"Batch=[{i + 1}/{len(train_dataloader)}]\")\n",
    "#     # break\n",
    "\n",
    "# for i, batch in enumerate(dev_dataloader):\n",
    "#     print(f\"Batch=[{i + 1}/{len(dev_dataloader)}]\")\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Longformer CRF model for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from transformers import LongformerModel\n",
    "\n",
    "\n",
    "class LongformerCRFForTokenClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, out_size, device, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_size = out_size\n",
    "        self.device = device\n",
    "\n",
    "        self.longformer = LongformerModel.from_pretrained(\n",
    "            pretrained_model_name, return_dict=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, out_size)\n",
    "\n",
    "        self.crf = CRF(num_tags=out_size, batch_first=True)\n",
    "\n",
    "        self.freeze_transformer_layer()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        sequence_output, _ = self.longformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            log_likelihood = self.crf(logits, labels)\n",
    "            logits = self.crf.decode(logits)\n",
    "\n",
    "            loss = 0 - log_likelihood\n",
    "        else:\n",
    "            logits = self.crf.decode(logits)\n",
    "        logits = torch.Tensor(logits).to(self.device)\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    def freeze_transformer_layer(self):\n",
    "        for param in self.longformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_transformer_layer(self):\n",
    "        # Fine-tune only the last 4 layer\n",
    "        for layer in self.longformer.encoder.layer[-4:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def get_predictions_from_logits(self, logits, labels=None, corresponding_word=None):\n",
    "        # logits: (batch_size, max_seq_len)\n",
    "        # labels: (batch_size, max_seq_len)\n",
    "        # corresponding_word: (batch_size, max_seq_len)\n",
    "\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "        # print(f\"logits: {logits}\")\n",
    "\n",
    "        # preds: (batch_size, max_seq_len)\n",
    "        # preds = torch.argmax(logits, dim=-1)\n",
    "        preds = logits.clone()\n",
    "\n",
    "        # print(f\"preds.shape: {preds.shape}\")\n",
    "        # print(f\"preds: {preds}\")\n",
    "\n",
    "        if labels is not None:\n",
    "            # print(f\"labels.shape: {labels.shape}\")\n",
    "            # print(f\"labels: {labels}\")\n",
    "\n",
    "            # Keep only predictions where labels are not -100\n",
    "            # clean_preds = preds[labels != -100].reshape(batch_size, -1)\n",
    "            # clean_labels = labels[labels != -100].reshape(batch_size, -1)\n",
    "\n",
    "            # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "            # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "            # print(f\"clean_labels.shape: {clean_labels.shape}\")\n",
    "            # print(f\"clean_labels: {clean_labels}\")\n",
    "\n",
    "            # Get the index of the first machine text word\n",
    "            # predicted_positions = clean_preds.argmax(dim=-1)\n",
    "            # true_positions = clean_labels.argmax(dim=-1)\n",
    "\n",
    "            predicted_positions = []\n",
    "            true_positions = []\n",
    "            for p, l in zip(preds, labels):\n",
    "                mask = l != -100\n",
    "\n",
    "                clean_pred = p[mask]\n",
    "                clean_label = l[mask]\n",
    "\n",
    "                # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "                # print(f\"clean_pred: {clean_pred}\")\n",
    "                # print(f\"clean_label.shape: {clean_label.shape}\")\n",
    "                # print(f\"clean_label: {clean_label}\")\n",
    "\n",
    "                predicted_position = clean_pred.argmax(dim=-1)\n",
    "                true_position = clean_label.argmax(dim=-1)\n",
    "\n",
    "                # print(f\"predicted_position: {predicted_position}\")\n",
    "                # print(f\"true_position: {true_position}\")\n",
    "\n",
    "                predicted_positions.append(predicted_position.item())\n",
    "                true_positions.append(true_position.item())\n",
    "\n",
    "            # print(f\"predicted_positions.shape: {predicted_positions.shape}\")\n",
    "            # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "            # print(f\"true_positions.shape: {true_positions.shape}\")\n",
    "            # print(f\"true_positions: {true_positions}\")\n",
    "\n",
    "            # print(f\"predicted_positions type: {type(predicted_positions)}\")\n",
    "            # print(f\"true_positions type: {type(true_positions)}\")\n",
    "\n",
    "            return torch.Tensor(predicted_positions), torch.Tensor(true_positions)\n",
    "        elif corresponding_word is not None:\n",
    "            # print(f\"corresponding_word.shape: {corresponding_word.shape}\")\n",
    "            # print(f\"corresponding_word: {corresponding_word}\")\n",
    "\n",
    "            # Keep only predictions where corresponding_word are not -100\n",
    "            # clean_preds = preds[corresponding_word != -100].reshape(\n",
    "            #     batch_size, -1\n",
    "            # ).detach().cpu().numpy()\n",
    "            # clean_corresponding_word = corresponding_word[corresponding_word != -100].reshape(\n",
    "            #     batch_size, -1\n",
    "            # ).detach().cpu().numpy()\n",
    "\n",
    "            # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "            # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "            # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "            # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "            predicted_positions = []\n",
    "            for p, w in zip(preds, corresponding_word):\n",
    "                mask = w != -100\n",
    "\n",
    "                clean_pred = p[mask]\n",
    "                clean_corresponding_word = w[mask]\n",
    "\n",
    "                # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "                # print(f\"clean_pred: {clean_pred}\")\n",
    "                # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "                # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "                # Get the index of the first machine text word\n",
    "                index = torch.where(clean_pred == 1)[0]\n",
    "                value = index[0] if index.size else len(clean_pred) - 1\n",
    "                position = clean_corresponding_word[value]\n",
    "\n",
    "                # print(f\"index: {index}\")\n",
    "                # print(f\"value: {value}\")\n",
    "                # print(f\"position: {position}\")\n",
    "\n",
    "                predicted_positions.append(position.item())\n",
    "            #     # pred = pred.detach().cpu().numpy()\n",
    "\n",
    "            #     index = np.where(pred == 1)[0]\n",
    "            #     value = index[0] if index.size else len(pred) - 1\n",
    "            #     position = clean_corresponding_word[idx][value]\n",
    "\n",
    "            #     predicted_positions.append(position.item())\n",
    "\n",
    "            print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "            return predicted_positions, None\n",
    "        else:\n",
    "            raise ValueError(\"Either labels or corresponding_word must be provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from time import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    metric_fn,\n",
    "    print_freq=10,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    all_ids = []\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        ids = batch[\"id\"]\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        corresponding_word = batch[\"corresponding_word\"].to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=targets,\n",
    "        )\n",
    "\n",
    "        predictions, true_predictions = model.get_predictions_from_logits(\n",
    "            logits=logits,\n",
    "            labels=targets,\n",
    "            corresponding_word=corresponding_word\n",
    "        )\n",
    "\n",
    "        # print(f\"predictions: {predictions}\")\n",
    "        # print(f\"true_predictions: {true_predictions}\")\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "        all_true.extend(true_predictions.tolist())\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print(\n",
    "                f\"Batch [{i + 1}/{len(dataloader)}]; \"\n",
    "                f\"Loss: {loss.item():.5f}; \"\n",
    "                f\"Mean absolute error: {metric_fn(true_predictions, predictions):.5f}\"\n",
    "            )\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return np.mean(losses), (all_ids, all_true, all_predictions)\n",
    "\n",
    "\n",
    "def validation_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    metric_fn,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            ids = batch[\"id\"]\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            corresponding_word = batch[\"corresponding_word\"].to(device)\n",
    "\n",
    "            loss, logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets,\n",
    "            )\n",
    "\n",
    "            predictions, true_predictions = model.get_predictions_from_logits(\n",
    "                logits=logits,\n",
    "                labels=targets,\n",
    "                corresponding_word=corresponding_word\n",
    "            )\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "            all_true.extend(true_predictions.tolist())\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    return np.mean(losses), (all_ids, all_true, all_predictions)\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    model,\n",
    "    num_epochs,\n",
    "    train_dataloader,\n",
    "    dev_dataloader,\n",
    "    loss_fn,\n",
    "    optimizer_config,\n",
    "    scheduler_config,\n",
    "    device,\n",
    "    metric_fn,\n",
    "    is_better_metric_fn,\n",
    "    num_epochs_before_finetune,\n",
    "    results_dir,\n",
    "):\n",
    "    history = defaultdict(list)\n",
    "    best_metric = None\n",
    "    best_model_state = None\n",
    "\n",
    "    optimizer = get_optimizer(model, optimizer_config, finetune=False)\n",
    "    scheduler = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        if epoch <= num_epochs_before_finetune:\n",
    "            print(\"Freeze transformer\")\n",
    "        else:\n",
    "            print(\"Finetune transformer\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        if epoch == num_epochs_before_finetune + 1:\n",
    "            model.unfreeze_transformer_layer()\n",
    "            optimizer = get_optimizer(model, optimizer_config, finetune=True)\n",
    "            scheduler = get_scheduler(\n",
    "                optimizer,\n",
    "                num_training_steps=len(train_dataloader) * num_epochs,\n",
    "                **scheduler_config,\n",
    "            )\n",
    "\n",
    "        train_loss, (train_ids, train_true, train_predict) = train_epoch(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            metric_fn,\n",
    "        )\n",
    "\n",
    "        train_metric = metric_fn(train_true, train_predict)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.5f}; Train Metric: {train_metric:.5f}\")\n",
    "\n",
    "        dev_loss, (dev_ids, dev_true, dev_predict) = validation_epoch(\n",
    "            model,\n",
    "            dev_dataloader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            metric_fn,\n",
    "        )\n",
    "\n",
    "        dev_metric = metric_fn(dev_true, dev_predict)\n",
    "\n",
    "        print(\n",
    "            f\"Validation Loss: {dev_loss:.5f}; \"\n",
    "            f\"Validation Metric: {dev_metric:.5f}\"\n",
    "        )\n",
    "\n",
    "        history[\"train_metric\"].append(train_metric)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"dev_metric\"].append(dev_metric)\n",
    "        history[\"dev_loss\"].append(dev_loss)\n",
    "\n",
    "        if best_metric is None or is_better_metric_fn(train_metric, best_metric):\n",
    "            best_metric = train_metric\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "            if results_dir is not None:\n",
    "                torch.save(\n",
    "                    best_model_state,\n",
    "                    os.path.join(results_dir, \"best_model.bin\"),\n",
    "                )\n",
    "\n",
    "                df_train_predictions = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": train_ids,\n",
    "                        \"true\": train_true,\n",
    "                        \"predict\": train_predict,\n",
    "                    }\n",
    "                )\n",
    "                df_train_predictions.to_csv(\n",
    "                    os.path.join(results_dir, \"best_model_train_predict.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "\n",
    "                df_dev_predictions = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": dev_ids,\n",
    "                        \"true\": dev_true,\n",
    "                        \"predict\": dev_predict,\n",
    "                    }\n",
    "                )\n",
    "                df_dev_predictions.to_csv(\n",
    "                    os.path.join(results_dir, \"best_model_dev_predict.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "\n",
    "    df_history = pd.DataFrame(history)\n",
    "    if results_dir is not None:\n",
    "        df_history.to_csv(os.path.join(results_dir, \"history.csv\"), index=False)\n",
    "\n",
    "        model.load_state_dict(torch.load(os.path.join(results_dir, \"best_model.bin\")))\n",
    "    else:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Finetune transformer\n",
      "----------\n",
      "Batch [1/183]; Loss: nan; Mean absolute error: 80.87500\n",
      "Batch [11/183]; Loss: 9766.71289; Mean absolute error: 143.00000\n",
      "Batch [21/183]; Loss: nan; Mean absolute error: 107.81250\n",
      "Batch [31/183]; Loss: nan; Mean absolute error: 129.00000\n",
      "Batch [41/183]; Loss: 8029.16113; Mean absolute error: 72.50000\n",
      "Batch [51/183]; Loss: nan; Mean absolute error: 86.31250\n",
      "Batch [61/183]; Loss: nan; Mean absolute error: 107.31250\n",
      "Batch [71/183]; Loss: 6612.65039; Mean absolute error: 134.31250\n",
      "Batch [81/183]; Loss: nan; Mean absolute error: 95.43750\n",
      "Batch [91/183]; Loss: nan; Mean absolute error: 54.75000\n",
      "Batch [101/183]; Loss: 5013.09814; Mean absolute error: 34.81250\n",
      "Batch [111/183]; Loss: nan; Mean absolute error: 32.75000\n",
      "Batch [121/183]; Loss: nan; Mean absolute error: 54.00000\n",
      "Batch [131/183]; Loss: 7236.47559; Mean absolute error: 52.18750\n",
      "Batch [141/183]; Loss: nan; Mean absolute error: 34.93750\n",
      "Batch [151/183]; Loss: nan; Mean absolute error: 69.18750\n",
      "Batch [161/183]; Loss: 5509.93359; Mean absolute error: 23.75000\n",
      "Batch [171/183]; Loss: nan; Mean absolute error: 31.00000\n",
      "Batch [181/183]; Loss: nan; Mean absolute error: 32.12500\n",
      "Train Loss: nan; Train Metric: 76.19904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [04:58<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: nan; Validation Metric: 64.01507\n",
      "Epoch 2/5\n",
      "Finetune transformer\n",
      "----------\n",
      "Batch [1/183]; Loss: nan; Mean absolute error: 37.50000\n",
      "Batch [11/183]; Loss: 5534.94580; Mean absolute error: 35.50000\n",
      "Batch [21/183]; Loss: nan; Mean absolute error: 54.00000\n",
      "Batch [31/183]; Loss: nan; Mean absolute error: 27.43750\n",
      "Batch [41/183]; Loss: 4961.21582; Mean absolute error: 26.12500\n",
      "Batch [51/183]; Loss: nan; Mean absolute error: 14.37500\n",
      "Batch [61/183]; Loss: nan; Mean absolute error: 66.25000\n",
      "Batch [71/183]; Loss: 3124.94531; Mean absolute error: 45.00000\n",
      "Batch [81/183]; Loss: nan; Mean absolute error: 25.25000\n",
      "Batch [91/183]; Loss: nan; Mean absolute error: 19.43750\n",
      "Batch [101/183]; Loss: 5074.36523; Mean absolute error: 23.50000\n",
      "Batch [111/183]; Loss: nan; Mean absolute error: 21.81250\n",
      "Batch [121/183]; Loss: nan; Mean absolute error: 32.37500\n",
      "Batch [131/183]; Loss: 3878.21191; Mean absolute error: 32.56250\n",
      "Batch [141/183]; Loss: nan; Mean absolute error: 7.75000\n",
      "Batch [151/183]; Loss: nan; Mean absolute error: 29.25000\n",
      "Batch [161/183]; Loss: 5270.49951; Mean absolute error: 54.81250\n",
      "Batch [171/183]; Loss: nan; Mean absolute error: 41.37500\n",
      "Batch [181/183]; Loss: 3763.43018; Mean absolute error: 26.68750\n",
      "Train Loss: nan; Train Metric: 35.69544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [04:51<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: nan; Validation Metric: 45.69178\n",
      "Epoch 3/5\n",
      "Finetune transformer\n",
      "----------\n",
      "Batch [1/183]; Loss: nan; Mean absolute error: 34.06250\n",
      "Batch [11/183]; Loss: 6530.47705; Mean absolute error: 22.43750\n",
      "Batch [21/183]; Loss: nan; Mean absolute error: 16.62500\n",
      "Batch [31/183]; Loss: nan; Mean absolute error: 34.18750\n",
      "Batch [41/183]; Loss: 5435.17969; Mean absolute error: 27.25000\n",
      "Batch [51/183]; Loss: nan; Mean absolute error: 25.75000\n",
      "Batch [61/183]; Loss: nan; Mean absolute error: 26.50000\n",
      "Batch [71/183]; Loss: 4317.05420; Mean absolute error: 10.50000\n",
      "Batch [81/183]; Loss: nan; Mean absolute error: 28.68750\n",
      "Batch [91/183]; Loss: nan; Mean absolute error: 22.12500\n",
      "Batch [101/183]; Loss: 5348.78711; Mean absolute error: 30.37500\n",
      "Batch [111/183]; Loss: nan; Mean absolute error: 18.37500\n",
      "Batch [121/183]; Loss: nan; Mean absolute error: 37.18750\n",
      "Batch [131/183]; Loss: 5803.58398; Mean absolute error: 13.31250\n",
      "Batch [141/183]; Loss: nan; Mean absolute error: 20.31250\n",
      "Batch [151/183]; Loss: nan; Mean absolute error: 33.50000\n",
      "Batch [161/183]; Loss: 2158.16211; Mean absolute error: 38.00000\n",
      "Batch [171/183]; Loss: nan; Mean absolute error: 36.75000\n",
      "Batch [181/183]; Loss: nan; Mean absolute error: 29.87500\n",
      "Train Loss: nan; Train Metric: 30.15450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [04:54<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: nan; Validation Metric: 43.26438\n",
      "Epoch 4/5\n",
      "Finetune transformer\n",
      "----------\n",
      "Batch [1/183]; Loss: nan; Mean absolute error: 23.31250\n",
      "Batch [11/183]; Loss: 2062.53223; Mean absolute error: 25.18750\n",
      "Batch [21/183]; Loss: nan; Mean absolute error: 44.43750\n",
      "Batch [31/183]; Loss: nan; Mean absolute error: 17.75000\n",
      "Batch [41/183]; Loss: 2100.91797; Mean absolute error: 25.93750\n",
      "Batch [51/183]; Loss: nan; Mean absolute error: 19.18750\n",
      "Batch [61/183]; Loss: nan; Mean absolute error: 44.62500\n",
      "Batch [71/183]; Loss: 4257.60352; Mean absolute error: 27.50000\n",
      "Batch [81/183]; Loss: nan; Mean absolute error: 29.50000\n",
      "Batch [91/183]; Loss: nan; Mean absolute error: 43.56250\n",
      "Batch [101/183]; Loss: 2088.25195; Mean absolute error: 27.56250\n",
      "Batch [111/183]; Loss: nan; Mean absolute error: 18.56250\n",
      "Batch [121/183]; Loss: nan; Mean absolute error: 22.00000\n",
      "Batch [131/183]; Loss: 4261.31006; Mean absolute error: 28.06250\n",
      "Batch [141/183]; Loss: nan; Mean absolute error: 37.75000\n",
      "Batch [151/183]; Loss: nan; Mean absolute error: 16.43750\n",
      "Batch [161/183]; Loss: 4039.89160; Mean absolute error: 27.31250\n",
      "Batch [171/183]; Loss: nan; Mean absolute error: 21.37500\n",
      "Batch [181/183]; Loss: nan; Mean absolute error: 16.18750\n",
      "Train Loss: nan; Train Metric: 27.99589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [04:59<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: nan; Validation Metric: 38.36575\n",
      "Epoch 5/5\n",
      "Finetune transformer\n",
      "----------\n",
      "Batch [1/183]; Loss: nan; Mean absolute error: 24.93750\n",
      "Batch [11/183]; Loss: 2879.02881; Mean absolute error: 34.31250\n",
      "Batch [21/183]; Loss: nan; Mean absolute error: 18.31250\n",
      "Batch [31/183]; Loss: nan; Mean absolute error: 30.68750\n",
      "Batch [41/183]; Loss: 4056.57812; Mean absolute error: 25.56250\n",
      "Batch [51/183]; Loss: nan; Mean absolute error: 22.81250\n",
      "Batch [61/183]; Loss: nan; Mean absolute error: 31.68750\n",
      "Batch [71/183]; Loss: 2102.71484; Mean absolute error: 23.62500\n",
      "Batch [81/183]; Loss: nan; Mean absolute error: 23.93750\n",
      "Batch [91/183]; Loss: nan; Mean absolute error: 20.37500\n",
      "Batch [101/183]; Loss: 2399.46436; Mean absolute error: 28.31250\n",
      "Batch [111/183]; Loss: nan; Mean absolute error: 70.68750\n",
      "Batch [121/183]; Loss: nan; Mean absolute error: 18.87500\n",
      "Batch [131/183]; Loss: 4223.10938; Mean absolute error: 68.12500\n",
      "Batch [141/183]; Loss: nan; Mean absolute error: 20.18750\n",
      "Batch [151/183]; Loss: nan; Mean absolute error: 28.50000\n",
      "Batch [161/183]; Loss: 5040.05566; Mean absolute error: 22.75000\n",
      "Batch [171/183]; Loss: nan; Mean absolute error: 33.43750\n",
      "Batch [181/183]; Loss: nan; Mean absolute error: 21.81250\n",
      "Train Loss: nan; Train Metric: 25.69544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [04:55<00:00,  6.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: nan; Validation Metric: 38.70274\n"
     ]
    }
   ],
   "source": [
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "model = LongformerCRFForTokenClassification(\n",
    "    device=DEVICE, **config[\"model_config\"]\n",
    ").to(DEVICE)\n",
    "loss_fn = get_loss_fn(config[\"training\"][\"loss\"], DEVICE)\n",
    "optimizer_config = config[\"training\"][\"optimizer\"]\n",
    "scheduler_config = config[\"training\"][\"scheduler\"]\n",
    "metric_fn, is_better_metric_fn = get_metric(config[\"training\"][\"metric\"])\n",
    "num_epochs_before_finetune = config[\"training\"][\"num_epochs_before_finetune\"]\n",
    "\n",
    "best_model, df_history = training_loop(\n",
    "    model,\n",
    "    num_epochs,\n",
    "    train_dataloader,\n",
    "    dev_dataloader,\n",
    "    loss_fn,\n",
    "    optimizer_config,\n",
    "    scheduler_config,\n",
    "    DEVICE,\n",
    "    metric_fn,\n",
    "    is_better_metric_fn,\n",
    "    num_epochs_before_finetune,\n",
    "    results_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    results_dir,\n",
    "    label_column,\n",
    "    file_format=\"csv\",\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            ids = batch[\"id\"]\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            corresponding_word = batch[\"corresponding_word\"].to(device)\n",
    "\n",
    "            _, logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets,\n",
    "            )\n",
    "\n",
    "            predictions, true_predictions = model.get_predictions_from_logits(\n",
    "                logits=logits,\n",
    "                labels=targets,\n",
    "                corresponding_word=corresponding_word\n",
    "            )\n",
    "\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "            all_true.extend(true_predictions.tolist())\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    df_predictions = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": all_ids,\n",
    "            \"true\": all_true,\n",
    "            label_column: all_predictions,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if results_dir is not None:\n",
    "        if file_format == \"csv\":\n",
    "            df_predictions.to_csv(\n",
    "                os.path.join(results_dir, \"submission.csv\"),\n",
    "                index=False,\n",
    "            )\n",
    "        elif file_format == \"jsonl\":\n",
    "            df_predictions.to_json(\n",
    "                os.path.join(results_dir, \"submission.jsonl\"),\n",
    "                orient=\"records\",\n",
    "                lines=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown file format: {file_format}\")\n",
    "    else:\n",
    "        print(\"Missing results_dir, not saving predictions to file!\")\n",
    "\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [03:22<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = make_predictions(\n",
    "    best_model,\n",
    "    test_dataloader,\n",
    "    DEVICE,\n",
    "    results_dir,\n",
    "    config[\"data\"][\"label_column\"],\n",
    "    file_format=\"csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation\n",
      "MAE: 38.70274\n",
      "--------------------\n",
      "Results on test\n",
      "MAE: 32.22178\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../scores_and_plots.py --results-dir \"../runs/SubtaskC\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
