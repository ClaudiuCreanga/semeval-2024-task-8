{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmarchitan/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from lib.utils import get_device, get_current_date\n",
    "from lib.utils.constants import Subtask, Track, PreprocessTextLevel, DatasetType\n",
    "from lib.utils.models import sequential_fully_connected\n",
    "from lib.data.loading import load_train_dev_test_df, build_data_loader\n",
    "from lib.data.tokenizer import get_tokenizer\n",
    "from lib.data.vocabulary import get_vocabulary, WordVocabulary, CharacterVocabulary\n",
    "from lib.training.optimizer import get_optimizer, get_scheduler\n",
    "from lib.training.loss import get_loss_fn\n",
    "from lib.training.metric import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE_PATH = os.path.relpath(\"../config.json\")\n",
    "\n",
    "config = {}\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Track not specified in config for subtask: Subtask.SubtaskC\n",
      "Loading train data...\n",
      ".././data/original_data/SubtaskC/SubtaskC_train.jsonl\n",
      "df_train.shape: (3649, 3)\n",
      "df_dev.shape: (505, 3)\n",
      "df_test.shape: (11123, 2)\n"
     ]
    }
   ],
   "source": [
    "task = None\n",
    "if \"task\" in config:\n",
    "    task = Subtask(config[\"task\"])\n",
    "else:\n",
    "    raise ValueError(\"Task not specified in config\")\n",
    "\n",
    "track = None\n",
    "if \"track\" in config:\n",
    "    track = Track(config[\"track\"])\n",
    "else:\n",
    "    print(f\"Warning: Track not specified in config for subtask: {task}\")\n",
    "\n",
    "dataset_type = DatasetType.TransformerTruncationDataset\n",
    "if \"dataset_type\" in config[\"data\"]:\n",
    "    dataset_type = DatasetType(config[\"data\"][\"dataset_type\"])\n",
    "\n",
    "dataset_type_settings = None\n",
    "if \"dataset_type_settings\" in config[\"data\"]:\n",
    "    dataset_type_settings = config[\"data\"][\"dataset_type_settings\"]\n",
    "\n",
    "test_size = (\n",
    "    None if \"test_size\" not in config[\"data\"] else config[\"data\"][\"test_size\"]\n",
    ")\n",
    "df_train, df_dev, df_test = load_train_dev_test_df(\n",
    "    task=task,\n",
    "    track=track,\n",
    "    data_dir=f\"../{config['data']['data_dir']}\",\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    test_size=test_size,\n",
    "    preprocess_text_level=PreprocessTextLevel(\n",
    "        config[\"data\"][\"preprocess_text_level\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "print(f\"df_dev.shape: {df_dev.shape}\")\n",
    "print(f\"df_test.shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save results to: ../runs/20-01-2024_09:30:34-SubtaskC-cnn_bilstm_with_crf_for_token_classification\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    results_dir = os.path.relpath(\"../runs/SubtaskC/\")\n",
    "else:\n",
    "    results_dir = os.path.relpath(\n",
    "        f\"../runs/{get_current_date()}-{task.value}-{config['model']}\"\n",
    "    )\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "print(f\"Will save results to: {results_dir}\")\n",
    "\n",
    "with open(results_dir + \"/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(**config[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 3649/3649 [00:00<00:00, 41058.63it/s]\n",
      "Building word vocabulary: 100%|██████████| 3649/3649 [00:00<00:00, 50081.69it/s]\n"
     ]
    }
   ],
   "source": [
    "char_vocabulary, word_vocabulary = None, None\n",
    "char_max_len, word_max_len = None, config[\"data\"][\"max_len\"]\n",
    "if dataset_type == DatasetType.TokenClassificationDataset:\n",
    "    if dataset_type_settings is not None:\n",
    "        if \"chars\" in dataset_type_settings:\n",
    "            char_vocabulary = get_vocabulary(\"chars\")\n",
    "            char_vocabulary.build_vocabulary(df_train)\n",
    "\n",
    "            char_max_len = dataset_type_settings[\"chars\"][\"max_len\"]\n",
    "\n",
    "        if \"words\" in dataset_type_settings:\n",
    "            word_vocabulary = get_vocabulary(\"words\")\n",
    "            word_vocabulary.build_vocabulary(df_train)\n",
    "\n",
    "            word_max_len = dataset_type_settings[\"words\"][\"max_len\"]\n",
    "    else:\n",
    "        word_vocabulary = get_vocabulary(\"words\")\n",
    "        word_vocabulary.build_vocabulary(df_train)\n",
    "\n",
    "        word_max_len = config[\"data\"][\"max_len\"]\n",
    "\n",
    "if \"vocab_size\" in config[\"model_config\"]:\n",
    "    config[\"model_config\"][\"vocab_size\"] = word_vocabulary.vocab_size()\n",
    "if \"char_vocab_size\" in config[\"model_config\"]:\n",
    "    config[\"model_config\"][\"char_vocab_size\"] = char_vocabulary.vocab_size()\n",
    "if \"word_vocab_size\" in config[\"model_config\"]:\n",
    "    config[\"model_config\"][\"word_vocab_size\"] = word_vocabulary.vocab_size()\n",
    "\n",
    "if \"char_max_len\" in config[\"model_config\"]:\n",
    "    config[\"model_config\"][\"char_max_len\"] = char_max_len\n",
    "\n",
    "# Save vocab size to config\n",
    "with open(results_dir + \"/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = build_data_loader(\n",
    "    df_train,\n",
    "    tokenizer,\n",
    "    max_len=word_max_len,\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    shuffle=True,\n",
    "    dataset_type=dataset_type,\n",
    "    dataset_type_settings=dataset_type_settings,\n",
    "    char_vocabulary=char_vocabulary,\n",
    "    char_max_len=char_max_len,\n",
    "    word_vocabulary=word_vocabulary,\n",
    "    device=DEVICE,\n",
    ")\n",
    "dev_dataloader = build_data_loader(\n",
    "    df_dev,\n",
    "    tokenizer,\n",
    "    max_len=word_max_len,\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    dataset_type=dataset_type,\n",
    "    dataset_type_settings=dataset_type_settings,\n",
    "    char_vocabulary=char_vocabulary,\n",
    "    char_max_len=char_max_len,\n",
    "    word_vocabulary=word_vocabulary,\n",
    "    device=DEVICE,\n",
    ")\n",
    "test_dataloader = build_data_loader(\n",
    "    df_test,\n",
    "    tokenizer,\n",
    "    max_len=word_max_len,\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    has_targets=False if test_size is None else True,\n",
    "    dataset_type=dataset_type,\n",
    "    dataset_type_settings=dataset_type_settings,\n",
    "    char_vocabulary=char_vocabulary,\n",
    "    char_max_len=char_max_len,\n",
    "    word_vocabulary=word_vocabulary,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocabulary.idx2char[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocabulary.idx2word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"Batch=[{i + 1}/{len(train_dataloader)}]\")\n",
    "    print(f\"batch['input_ids'].shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"batch['char_input_ids'].shape: {batch['char_input_ids'].shape}\")\n",
    "    print(f\"batch['attention_mask'].shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"batch['char_attention_mask'].shape: {batch['char_attention_mask'].shape}\")\n",
    "    print(f\"batch['target'].shape: {batch['target'].shape}\")\n",
    "    print(f\"batch['target']: {batch['target']}\")\n",
    "    print(f\"batch['corresponding_word']: {batch['corresponding_word']}\")\n",
    "    break\n",
    "\n",
    "# for i, batch in enumerate(dev_dataloader):\n",
    "#     print(f\"Batch=[{i + 1}/{len(dev_dataloader)}]\")\n",
    "# #     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained FastText Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_pretrained_vectors(word2idx, pretrained_vectors_path, pad_token=\"<pad>\"):\n",
    "    fin = open(pretrained_vectors_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
    "    n, d = map(int, fin.readline().split())\n",
    "\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
    "    embeddings[word2idx[pad_token]] = np.zeros((d,))\n",
    "\n",
    "    count = 0\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(\" \")\n",
    "        word = tokens[0]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
    "\n",
    "    coverage = count / len(word2idx) * 100\n",
    "    print(f\"There are {count}/{len(word2idx)} pretraied vectors found ({coverage:.4f}%)\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999995it [00:17, 114150.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12694/29594 pretraied vectors found (42.8938%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_pretrained_vectors(\n",
    "    word_vocabulary.word2idx,\n",
    "    \"../data/fasttext/crawl-300d-2M.vec\",\n",
    "    pad_token=word_vocabulary.padding_token,\n",
    ")\n",
    "embeddings = torch.Tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocabulary.padding_token_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN model for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import LongformerModel\n",
    "\n",
    "try:\n",
    "    from torchcrf import CRF\n",
    "except ImportError:\n",
    "    print(f\"Warning: CRF module not found. Install it with pip install torchcrf\")\n",
    "\n",
    "# TODO: Implement this: https://medium.com/illuin/named-entity-recognition-with-bilstm-cnns-632ba83d3d41\n",
    "# Use the CNN as feature extractor for the character level\n",
    "# Use FastText or something similar for the word level embeddings\n",
    "# Combine them with BiLSTM\n",
    "\n",
    "class CharacterLevelCNNEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        embedding_dim,\n",
    "        filter_size=3,\n",
    "        num_filters=30,\n",
    "        dropout_p=0.5,\n",
    "    ):\n",
    "        super(CharacterLevelCNNEmbedding, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        # self.max_len = max_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.filter_size = filter_size\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=1,\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=filter_size,\n",
    "            padding=(filter_size - 1) // 2,\n",
    "        )\n",
    "\n",
    "        self.max_pool_1d = nn.MaxPool1d(kernel_size=max_len)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self._init_embedding_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # input_ids: (batch_size, max_seq_len, max_char_len)\n",
    "        # print(f\"input_ids.shape: {input_ids.shape}\")\n",
    "\n",
    "        outputs = []\n",
    "        for i, x in enumerate(input_ids):\n",
    "            # print(f\"Token {i + 1}/{input_ids.shape[0]}\")\n",
    "            # x.shape: (max_seq_len, max_char_len)\n",
    "\n",
    "            # embeddings.shape: (max_seq_len, max_char_len, embedding_dim)\n",
    "            embeddings = self.embedding(x)\n",
    "            # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "            embeddings = self.dropout(embeddings)\n",
    "            # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "            # embeddings.shape: (max_seq_len, embedding_dim, max_char_len)\n",
    "            embeddings = embeddings.permute(0, 2, 1)\n",
    "            # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "            # conv_out.shape: (max_seq_len, num_filters, max_char_len)\n",
    "            conv_out = F.tanh(self.conv(embeddings))\n",
    "            # print(f\"conv_out.shape: {conv_out.shape}\")\n",
    "\n",
    "            # max_pool_out.shape: (max_seq_len, num_filters, 1)\n",
    "            max_pool_out = self.max_pool_1d(conv_out)\n",
    "            # print(f\"max_pool_out.shape: {max_pool_out.shape}\")\n",
    "\n",
    "            # max_pool_out.shape: (max_seq_len, num_filters)\n",
    "            max_pool_out = max_pool_out.view(max_pool_out.size(0), -1)\n",
    "            # print(f\"max_pool_out.shape: {max_pool_out.shape}\")\n",
    "\n",
    "            # output.shape: (max_seq_len, num_filters)\n",
    "            output = self.dropout(max_pool_out)\n",
    "            # print(f\"output.shape: {output.shape}\")\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "        # outputs.shape: (batch_size, max_seq_len, num_filters)\n",
    "        outputs = torch.stack(outputs)\n",
    "        # print(f\"outputs.shape: {outputs.shape}\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _init_embedding_weights(self):\n",
    "        self.embedding.weight.data = self.embedding.weight.data.uniform_(\n",
    "            -0.5, 0.5\n",
    "        )\n",
    "\n",
    "\n",
    "class CNNBiLSTMForTokenClassification(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        char_vocab_size,\n",
    "        char_max_len,\n",
    "        char_embedding_dim,\n",
    "        char_filter_size=3,\n",
    "        char_num_filters=30,\n",
    "        char_dropout_p=0.5,\n",
    "        word_pretrained_embedding=None,\n",
    "        word_freeze_embedding=False,\n",
    "        word_vocab_size=None,\n",
    "        word_embedding_dim=300,\n",
    "        # filter_sizes=[3, 4, 5],\n",
    "        # num_filters=[100, 100, 100],\n",
    "        n_layers=1,\n",
    "        hidden_dim=32,\n",
    "        dropout_p=0.5,\n",
    "        fc=[],\n",
    "        out_size=2,\n",
    "    ):\n",
    "        super(CNNBiLSTMForTokenClassification, self).__init__()\n",
    "\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.char_embedding = CharacterLevelCNNEmbedding(\n",
    "            char_vocab_size,\n",
    "            char_max_len,\n",
    "            char_embedding_dim,\n",
    "            char_filter_size,\n",
    "            char_num_filters,\n",
    "            char_dropout_p,\n",
    "        )\n",
    "\n",
    "        if word_pretrained_embedding is not None:\n",
    "            self.word_vocab_size, self.word_embedding_dim = word_pretrained_embedding.shape\n",
    "            self.word_embedding = nn.Embedding.from_pretrained(\n",
    "                word_pretrained_embedding,\n",
    "                freeze=word_freeze_embedding,\n",
    "            )\n",
    "        else:\n",
    "            self.vocab_size = word_vocab_size\n",
    "            self.word_embedding_dim = word_embedding_dim\n",
    "            self.word_embedding = nn.Embedding(\n",
    "                num_embeddings=word_vocab_size,\n",
    "                embedding_dim=word_embedding_dim,\n",
    "                padding_idx=1,\n",
    "            )\n",
    "\n",
    "            self._init_embedding_weights()\n",
    "\n",
    "        # self.filter_sizes = filter_sizes\n",
    "        # self.num_filters = num_filters\n",
    "        # self.convs = nn.ModuleList(\n",
    "        #     nn.Conv1d(\n",
    "        #         in_channels=self.embed_dim,\n",
    "        #         out_channels=num_filters[i],\n",
    "        #         kernel_size=filter_sizes[i],\n",
    "        #         padding=(filter_sizes[i] - 1) // 2,\n",
    "        #     ) for i in range(len(filter_sizes))\n",
    "        # )\n",
    "\n",
    "        # self.fc = sequential_fully_connected(\n",
    "        #     np.sum(num_filters), out_size, fc, dropout_p,\n",
    "        # )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            char_num_filters + self.word_embedding_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # self.fc = nn.Linear(np.sum(num_filters), out_size)\n",
    "        self.classifier = sequential_fully_connected(\n",
    "            2 * hidden_dim, out_size, fc, dropout_p,\n",
    "        )\n",
    "        # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # self._compute_output_dim()\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "\n",
    "    def freeze_transformer_layer(self):\n",
    "        pass\n",
    "\n",
    "    def unfreeze_transformer_layer(self):\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        char_input_ids,\n",
    "        char_attention_mask,\n",
    "        device,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # input_ids: (batch_size, max_seq_len, max_char_len)\n",
    "        # print(f\"input_ids.shape: {input_ids.shape}\")\n",
    "\n",
    "        char_embeddings = self.char_embedding(\n",
    "            char_input_ids,\n",
    "            char_attention_mask,\n",
    "        )\n",
    "\n",
    "        # input_ids.shape: (batch_size, max_seq_len)\n",
    "        word_embeddings = self.word_embedding(input_ids)\n",
    "        # embeddings = embeddings.permute(0, 2, 1)\n",
    "        # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "        embeddings = torch.cat(\n",
    "            [char_embeddings, word_embeddings],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "        lengths = attention_mask.sum(dim=1)\n",
    "        # print(f\"lengths.shape: {lengths.shape}\")\n",
    "\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # print(f\"packed_embeddings.data.shape: {packed_embeddings.data.shape}\")\n",
    "\n",
    "        packed_output, (_, _) = self.lstm(packed_embeddings)\n",
    "        # print(f\"packed_output.data.shape: {packed_output.data.shape}\")\n",
    "\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=embeddings.shape[1],\n",
    "        )\n",
    "        # print(f\"output.shape: {output.shape}\")\n",
    "\n",
    "        # output = self.dropout(output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "            loss = loss_fn(logits.view(-1, self.out_size), labels.view(-1))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "        # x_conv.shape: (batch_size, num_filters[i], L_out)\n",
    "        # x_conv = [\n",
    "        #     F.relu(conv1d(embeddings)) for conv1d in self.convs\n",
    "        # ]\n",
    "        # for i in range(len(x_conv)):\n",
    "        #     print(f\"x_conv[{i}].shape: {x_conv[i].shape}\")\n",
    "        # print(f\"x_conv.shape: {x_conv[0].shape}\")\n",
    "        # print(f\"x_conv: {x_conv[0]}\")\n",
    "\n",
    "        # x_max_pool.shape: (batch_size, num_filters[i], 1)\n",
    "        # x_max_pool = [\n",
    "        #     F.max_pool1d(\n",
    "        #         conv,\n",
    "        #         kernel_size=self.filter_sizes[i],\n",
    "        #         stride=1,\n",
    "        #         padding=self.filter_sizes[i] // 2,\n",
    "        #     ) for i, conv in enumerate(x_conv)\n",
    "        # ]\n",
    "        # for i in range(len(x_max_pool)):\n",
    "        #     print(f\"x_max_pool[{i}].shape: {x_max_pool[i].shape}\")\n",
    "        # print(f\"x_max_pool.shape: {x_max_pool[0].shape}\")\n",
    "        # print(f\"x_max_pool: {x_max_pool[0]}\")\n",
    "\n",
    "        # x_fc = torch.cat(\n",
    "        #     x_max_pool,\n",
    "        #     dim=1,\n",
    "        # )\n",
    "        # x_fc = x_fc.permute(0, 2, 1)\n",
    "        # print(f\"x_fc.shape: {x_fc.shape}\")\n",
    "\n",
    "        # logits = self.fc(self.dropout(x_fc))\n",
    "        # logits = self.fc(x_fc)\n",
    "\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "\n",
    "        # return logits\n",
    "\n",
    "    def get_predictions_from_logits(self, logits, labels=None, corresponding_word=None):\n",
    "        # batch_size = logits.shape[0]\n",
    "\n",
    "        # logits: (batch_size, max_seq_len, out_size)\n",
    "        # labels: (batch_size, max_seq_len)\n",
    "        # corresponding_word: (batch_size, max_seq_len)\n",
    "\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "        # print(f\"logits: {logits}\")\n",
    "\n",
    "        # preds: (batch_size, max_seq_len)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # print(f\"preds.shape: {preds.shape}\")\n",
    "        # print(f\"preds: {preds}\")\n",
    "\n",
    "        if labels is not None:\n",
    "            # print(f\"labels.shape: {labels.shape}\")\n",
    "            # print(f\"labels: {labels}\")\n",
    "\n",
    "            # Keep only predictions where labels are not -100\n",
    "            # clean_preds = preds[labels != -100].reshape(batch_size, -1)\n",
    "            # clean_labels = labels[labels != -100].reshape(batch_size, -1)\n",
    "\n",
    "            # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "            # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "            # print(f\"clean_labels.shape: {clean_labels.shape}\")\n",
    "            # print(f\"clean_labels: {clean_labels}\")\n",
    "\n",
    "            # Get the index of the first machine text word\n",
    "            # predicted_positions = clean_preds.argmax(dim=-1)\n",
    "            # true_positions = clean_labels.argmax(dim=-1)\n",
    "\n",
    "            predicted_positions = []\n",
    "            true_positions = []\n",
    "            for p, l in zip(preds, labels):\n",
    "                mask = l != -100\n",
    "\n",
    "                clean_pred = p[mask]\n",
    "                clean_label = l[mask]\n",
    "\n",
    "                print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "                # print(f\"clean_pred: {clean_pred}\")\n",
    "                # print(f\"clean_label.shape: {clean_label.shape}\")\n",
    "                # print(f\"clean_label: {clean_label}\")\n",
    "\n",
    "                predicted_position = clean_pred.argmax(dim=-1)\n",
    "                true_position = clean_label.argmax(dim=-1)\n",
    "\n",
    "                # print(f\"predicted_position: {predicted_position}\")\n",
    "                # print(f\"true_position: {true_position}\")\n",
    "\n",
    "                predicted_positions.append(predicted_position.item())\n",
    "                true_positions.append(true_position.item())\n",
    "\n",
    "            # print(f\"predicted_positions.shape: {predicted_positions.shape}\")\n",
    "            # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "            # print(f\"true_positions.shape: {true_positions.shape}\")\n",
    "            # print(f\"true_positions: {true_positions}\")\n",
    "\n",
    "            # print(f\"predicted_positions type: {type(predicted_positions)}\")\n",
    "            # print(f\"true_positions type: {type(true_positions)}\")\n",
    "\n",
    "            return torch.Tensor(predicted_positions), torch.Tensor(true_positions)\n",
    "        elif corresponding_word is not None:\n",
    "            # print(f\"corresponding_word.shape: {corresponding_word.shape}\")\n",
    "            # print(f\"corresponding_word: {corresponding_word}\")\n",
    "\n",
    "            # Keep only predictions where corresponding_word are not -100\n",
    "            # clean_preds = preds[corresponding_word != -100].reshape(\n",
    "            #     batch_size, -1\n",
    "            # ).detach().cpu().numpy()\n",
    "            # clean_corresponding_word = corresponding_word[corresponding_word != -100].reshape(\n",
    "            #     batch_size, -1\n",
    "            # ).detach().cpu().numpy()\n",
    "\n",
    "            # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "            # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "            # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "            # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "            predicted_positions = []\n",
    "            for p, w in zip(preds, corresponding_word):\n",
    "                mask = w != -100\n",
    "\n",
    "                clean_pred = p[mask]\n",
    "                clean_corresponding_word = w[mask]\n",
    "\n",
    "                # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "                # print(f\"clean_pred: {clean_pred}\")\n",
    "                # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "                # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "                # Get the index of the first machine text word\n",
    "                index = torch.where(clean_pred == 1)[0]\n",
    "                value = index[0] if index.size else len(clean_pred) - 1\n",
    "                position = clean_corresponding_word[value]\n",
    "\n",
    "                # print(f\"index: {index}\")\n",
    "                # print(f\"value: {value}\")\n",
    "                # print(f\"position: {position}\")\n",
    "\n",
    "                predicted_positions.append(position.item())\n",
    "            #     # pred = pred.detach().cpu().numpy()\n",
    "\n",
    "            #     index = np.where(pred == 1)[0]\n",
    "            #     value = index[0] if index.size else len(pred) - 1\n",
    "            #     position = clean_corresponding_word[idx][value]\n",
    "\n",
    "            #     predicted_positions.append(position.item())\n",
    "\n",
    "            # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "            return predicted_positions, None\n",
    "        else:\n",
    "            raise ValueError(\"Either labels or corresponding_word must be provided\")\n",
    "\n",
    "    def _init_embedding_weights(self):\n",
    "        self.word_embedding.weight.data = self.word_embedding.weight.data.uniform_(\n",
    "            -0.5, 0.5\n",
    "        )\n",
    "\n",
    "\n",
    "class CNNBiLSTMWithCRFForTokenClassification(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        char_vocab_size,\n",
    "        char_max_len,\n",
    "        char_embedding_dim,\n",
    "        char_filter_size=3,\n",
    "        char_num_filters=30,\n",
    "        char_dropout_p=0.5,\n",
    "        word_pretrained_embedding=None,\n",
    "        word_freeze_embedding=False,\n",
    "        word_vocab_size=None,\n",
    "        word_embedding_dim=300,\n",
    "        # filter_sizes=[3, 4, 5],\n",
    "        # num_filters=[100, 100, 100],\n",
    "        n_layers=1,\n",
    "        hidden_dim=32,\n",
    "        dropout_p=0.5,\n",
    "        fc=[],\n",
    "        out_size=2,\n",
    "    ):\n",
    "        super(CNNBiLSTMWithCRFForTokenClassification, self).__init__()\n",
    "\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.char_embedding = CharacterLevelCNNEmbedding(\n",
    "            char_vocab_size,\n",
    "            char_max_len,\n",
    "            char_embedding_dim,\n",
    "            char_filter_size,\n",
    "            char_num_filters,\n",
    "            char_dropout_p,\n",
    "        )\n",
    "\n",
    "        if word_pretrained_embedding is not None:\n",
    "            self.word_vocab_size, self.word_embedding_dim = word_pretrained_embedding.shape\n",
    "            self.word_embedding = nn.Embedding.from_pretrained(\n",
    "                word_pretrained_embedding,\n",
    "                freeze=word_freeze_embedding,\n",
    "            )\n",
    "        else:\n",
    "            self.vocab_size = word_vocab_size\n",
    "            self.word_embedding_dim = word_embedding_dim\n",
    "            self.word_embedding = nn.Embedding(\n",
    "                num_embeddings=word_vocab_size,\n",
    "                embedding_dim=word_embedding_dim,\n",
    "                padding_idx=1,\n",
    "            )\n",
    "\n",
    "            self._init_embedding_weights()\n",
    "\n",
    "        # self.filter_sizes = filter_sizes\n",
    "        # self.num_filters = num_filters\n",
    "        # self.convs = nn.ModuleList(\n",
    "        #     nn.Conv1d(\n",
    "        #         in_channels=self.embed_dim,\n",
    "        #         out_channels=num_filters[i],\n",
    "        #         kernel_size=filter_sizes[i],\n",
    "        #         padding=(filter_sizes[i] - 1) // 2,\n",
    "        #     ) for i in range(len(filter_sizes))\n",
    "        # )\n",
    "\n",
    "        # self.fc = sequential_fully_connected(\n",
    "        #     np.sum(num_filters), out_size, fc, dropout_p,\n",
    "        # )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            char_num_filters + self.word_embedding_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # self.fc = nn.Linear(np.sum(num_filters), out_size)\n",
    "        self.classifier = sequential_fully_connected(\n",
    "            2 * hidden_dim, out_size, fc, dropout_p,\n",
    "        )\n",
    "        # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.crf = CRF(out_size, batch_first=True)\n",
    "\n",
    "        # self._compute_output_dim()\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "\n",
    "    def freeze_transformer_layer(self):\n",
    "        pass\n",
    "\n",
    "    def unfreeze_transformer_layer(self):\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        char_input_ids,\n",
    "        char_attention_mask,\n",
    "        device,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # input_ids: (batch_size, max_seq_len, max_char_len)\n",
    "        # print(f\"input_ids.shape: {input_ids.shape}\")\n",
    "\n",
    "        char_embeddings = self.char_embedding(\n",
    "            char_input_ids,\n",
    "            char_attention_mask,\n",
    "        )\n",
    "\n",
    "        # input_ids.shape: (batch_size, max_seq_len)\n",
    "        word_embeddings = self.word_embedding(input_ids)\n",
    "        # embeddings = embeddings.permute(0, 2, 1)\n",
    "        # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "        embeddings = torch.cat(\n",
    "            [char_embeddings, word_embeddings],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # print(f\"embeddings.shape: {embeddings.shape}\")\n",
    "\n",
    "        lengths = attention_mask.sum(dim=1)\n",
    "        # print(f\"lengths.shape: {lengths.shape}\")\n",
    "\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # print(f\"packed_embeddings.data.shape: {packed_embeddings.data.shape}\")\n",
    "\n",
    "        packed_output, (_, _) = self.lstm(packed_embeddings)\n",
    "        # print(f\"packed_output.data.shape: {packed_output.data.shape}\")\n",
    "\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=embeddings.shape[1],\n",
    "        )\n",
    "        # print(f\"output.shape: {output.shape}\")\n",
    "\n",
    "        # output = self.dropout(output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        mask = attention_mask.bool()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # print(\"YAY1\")\n",
    "            log_likelihood = self.crf(logits, labels, mask=mask)\n",
    "            logits = self.crf.decode(logits, mask=mask)\n",
    "\n",
    "            loss = 0 - log_likelihood\n",
    "        else:\n",
    "            # print(\"YAY2\")\n",
    "            logits = self.crf.decode(logits, mask=mask)\n",
    "\n",
    "        for i in range(len(logits)):\n",
    "            if len(logits[i]) < len(attention_mask[i]):\n",
    "                logits[i] = (\n",
    "                    [-100]\n",
    "                    + logits[i]\n",
    "                    + [-100] * (len(attention_mask[i]) - len(logits[i]) - 1)\n",
    "                )\n",
    "\n",
    "        # print(len(logits))\n",
    "        # print(logits)\n",
    "        logits = torch.Tensor(logits).to(device)\n",
    "        # print(logits.shape)\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "        # x_conv.shape: (batch_size, num_filters[i], L_out)\n",
    "        # x_conv = [\n",
    "        #     F.relu(conv1d(embeddings)) for conv1d in self.convs\n",
    "        # ]\n",
    "        # for i in range(len(x_conv)):\n",
    "        #     print(f\"x_conv[{i}].shape: {x_conv[i].shape}\")\n",
    "        # print(f\"x_conv.shape: {x_conv[0].shape}\")\n",
    "        # print(f\"x_conv: {x_conv[0]}\")\n",
    "\n",
    "        # x_max_pool.shape: (batch_size, num_filters[i], 1)\n",
    "        # x_max_pool = [\n",
    "        #     F.max_pool1d(\n",
    "        #         conv,\n",
    "        #         kernel_size=self.filter_sizes[i],\n",
    "        #         stride=1,\n",
    "        #         padding=self.filter_sizes[i] // 2,\n",
    "        #     ) for i, conv in enumerate(x_conv)\n",
    "        # ]\n",
    "        # for i in range(len(x_max_pool)):\n",
    "        #     print(f\"x_max_pool[{i}].shape: {x_max_pool[i].shape}\")\n",
    "        # print(f\"x_max_pool.shape: {x_max_pool[0].shape}\")\n",
    "        # print(f\"x_max_pool: {x_max_pool[0]}\")\n",
    "\n",
    "        # x_fc = torch.cat(\n",
    "        #     x_max_pool,\n",
    "        #     dim=1,\n",
    "        # )\n",
    "        # x_fc = x_fc.permute(0, 2, 1)\n",
    "        # print(f\"x_fc.shape: {x_fc.shape}\")\n",
    "\n",
    "        # logits = self.fc(self.dropout(x_fc))\n",
    "        # logits = self.fc(x_fc)\n",
    "\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "\n",
    "        # return logits\n",
    "\n",
    "    def get_predictions_from_logits(self, logits, labels=None, corresponding_word=None):\n",
    "        # batch_size = logits.shape[0]\n",
    "\n",
    "        # logits: (batch_size, max_seq_len, out_size)\n",
    "        # labels: (batch_size, max_seq_len)\n",
    "        # corresponding_word: (batch_size, max_seq_len)\n",
    "\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "        # print(f\"logits: {logits}\")\n",
    "\n",
    "        # preds: (batch_size, max_seq_len)\n",
    "        # preds = torch.argmax(logits, dim=-1)\n",
    "        preds = logits.clone()\n",
    "\n",
    "        # print(f\"preds.shape: {preds.shape}\")\n",
    "        # print(f\"preds: {preds}\")\n",
    "\n",
    "        if labels is not None:\n",
    "            # print(f\"labels.shape: {labels.shape}\")\n",
    "            # print(f\"labels: {labels}\")\n",
    "\n",
    "            # Keep only predictions where labels are not -100\n",
    "            # clean_preds = preds[labels != -100].reshape(batch_size, -1)\n",
    "            # clean_labels = labels[labels != -100].reshape(batch_size, -1)\n",
    "\n",
    "            # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "            # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "            # print(f\"clean_labels.shape: {clean_labels.shape}\")\n",
    "            # print(f\"clean_labels: {clean_labels}\")\n",
    "\n",
    "            # Get the index of the first machine text word\n",
    "            # predicted_positions = clean_preds.argmax(dim=-1)\n",
    "            # true_positions = clean_labels.argmax(dim=-1)\n",
    "\n",
    "            predicted_positions = []\n",
    "            true_positions = []\n",
    "            for p, l in zip(preds, labels):\n",
    "                mask = l != -100\n",
    "\n",
    "                clean_pred = p[mask]\n",
    "                clean_label = l[mask]\n",
    "\n",
    "                # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "                # print(f\"clean_pred: {clean_pred}\")\n",
    "                # print(f\"clean_label.shape: {clean_label.shape}\")\n",
    "                # print(f\"clean_label: {clean_label}\")\n",
    "\n",
    "                predicted_position = clean_pred.argmax(dim=-1)\n",
    "                true_position = clean_label.argmax(dim=-1)\n",
    "\n",
    "                # print(f\"predicted_position: {predicted_position}\")\n",
    "                # print(f\"true_position: {true_position}\")\n",
    "\n",
    "                predicted_positions.append(predicted_position.item())\n",
    "                true_positions.append(true_position.item())\n",
    "\n",
    "            # print(f\"predicted_positions.shape: {predicted_positions.shape}\")\n",
    "            # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "            # print(f\"true_positions.shape: {true_positions.shape}\")\n",
    "            # print(f\"true_positions: {true_positions}\")\n",
    "\n",
    "            # print(f\"predicted_positions type: {type(predicted_positions)}\")\n",
    "            # print(f\"true_positions type: {type(true_positions)}\")\n",
    "\n",
    "            return torch.Tensor(predicted_positions), torch.Tensor(true_positions)\n",
    "        elif corresponding_word is not None:\n",
    "            # print(f\"corresponding_word.shape: {corresponding_word.shape}\")\n",
    "            # print(f\"corresponding_word: {corresponding_word}\")\n",
    "\n",
    "            # Keep only predictions where corresponding_word are not -100\n",
    "            # clean_preds = preds[corresponding_word != -100].reshape(\n",
    "            #     batch_size, -1\n",
    "            # ).detach().cpu().numpy()\n",
    "            # clean_corresponding_word = corresponding_word[corresponding_word != -100].reshape(\n",
    "            #     batch_size, -1\n",
    "            # ).detach().cpu().numpy()\n",
    "\n",
    "            # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "            # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "            # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "            # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "            predicted_positions = []\n",
    "            for p, w in zip(preds, corresponding_word):\n",
    "                mask = w != -100\n",
    "\n",
    "                clean_pred = p[mask]\n",
    "                clean_corresponding_word = w[mask]\n",
    "\n",
    "                # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "                # print(f\"clean_pred: {clean_pred}\")\n",
    "                # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "                # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "                # Get the index of the first machine text word\n",
    "                index = torch.where(clean_pred == 1)[0]\n",
    "                value = index[0] if index.size else len(clean_pred) - 1\n",
    "                position = clean_corresponding_word[value]\n",
    "\n",
    "                # print(f\"index: {index}\")\n",
    "                # print(f\"value: {value}\")\n",
    "                # print(f\"position: {position}\")\n",
    "\n",
    "                predicted_positions.append(position.item())\n",
    "            #     # pred = pred.detach().cpu().numpy()\n",
    "\n",
    "            #     index = np.where(pred == 1)[0]\n",
    "            #     value = index[0] if index.size else len(pred) - 1\n",
    "            #     position = clean_corresponding_word[idx][value]\n",
    "\n",
    "            #     predicted_positions.append(position.item())\n",
    "\n",
    "            # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "            return torch.Tensor(predicted_positions), torch.Tensor([-1] * len(predicted_positions))\n",
    "        else:\n",
    "            raise ValueError(\"Either labels or corresponding_word must be provided\")\n",
    "\n",
    "    def _init_embedding_weights(self):\n",
    "        self.word_embedding.weight.data = self.word_embedding.weight.data.uniform_(\n",
    "            -0.5, 0.5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch=[1/305]\n",
      "loss: 2199.6484375\n"
     ]
    }
   ],
   "source": [
    "# char_cnn_embedding = CharacterLevelCNNEmbedding(\n",
    "#     vocab_size=char_vocabulary.vocab_size(),\n",
    "#     max_len=char_max_len,\n",
    "#     embedding_dim=10,\n",
    "#     filter_size=3,\n",
    "#     num_filters=30,\n",
    "#     dropout_p=0.5,\n",
    "# ).to(DEVICE)\n",
    "model = CNNBiLSTMWithCRFForTokenClassification(\n",
    "    char_vocab_size=char_vocabulary.vocab_size(),\n",
    "    char_max_len=char_max_len,\n",
    "    char_embedding_dim=10,\n",
    "    char_filter_size=3,\n",
    "    char_num_filters=30,\n",
    "    char_dropout_p=0.5,\n",
    "    word_pretrained_embedding=None,\n",
    "    word_freeze_embedding=False,\n",
    "    word_vocab_size=word_vocabulary.vocab_size(),\n",
    "    word_embedding_dim=50,\n",
    "    n_layers=1,\n",
    "    hidden_dim=32,\n",
    "    dropout_p=0.5,\n",
    "    fc=[16],\n",
    "    out_size=2,\n",
    ").to(DEVICE)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"Batch=[{i + 1}/{len(train_dataloader)}]\")\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "    char_input_ids = batch[\"char_input_ids\"].to(DEVICE)\n",
    "    char_attention_mask = batch[\"char_attention_mask\"].to(DEVICE)\n",
    "    labels = batch[\"target\"].to(DEVICE)\n",
    "\n",
    "    # print(f\"char_input_ids.shape: {char_input_ids.shape}\")\n",
    "    loss, output = model(\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        char_input_ids,\n",
    "        char_attention_mask,\n",
    "        DEVICE,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    print(f\"loss: {loss}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from time import time\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    metric_fn,\n",
    "    print_freq=10,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    all_ids = []\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        ids = batch[\"id\"]\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        char_input_ids = batch[\"char_input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        char_attention_mask = batch[\"char_attention_mask\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "        corresponding_word = batch[\"corresponding_word\"].to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            char_input_ids=char_input_ids,\n",
    "            char_attention_mask=char_attention_mask,\n",
    "            device=device,\n",
    "            labels=targets,\n",
    "        )\n",
    "\n",
    "        predictions, true_predictions = model.get_predictions_from_logits(\n",
    "            logits=logits,\n",
    "            labels=targets,\n",
    "            corresponding_word=corresponding_word\n",
    "        )\n",
    "\n",
    "        # loss = loss_fn(predictions, true_predictions)\n",
    "        # loss = Variable(loss, requires_grad=True)\n",
    "\n",
    "        # print(f\"predictions: {predictions}\")\n",
    "        # print(f\"true_predictions: {true_predictions}\")\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "        all_true.extend(true_predictions.tolist())\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print(\n",
    "                f\"Batch [{i + 1}/{len(dataloader)}]; \"\n",
    "                f\"Loss: {loss.item():.5f}; \"\n",
    "                f\"Mean absolute error: {metric_fn(true_predictions, predictions):.5f}\"\n",
    "            )\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return np.mean(losses), (all_ids, all_true, all_predictions)\n",
    "\n",
    "\n",
    "def validation_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    metric_fn,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            ids = batch[\"id\"]\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            char_input_ids = batch[\"char_input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            char_attention_mask = batch[\"char_attention_mask\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            corresponding_word = batch[\"corresponding_word\"].to(device)\n",
    "\n",
    "            loss, logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                char_input_ids=char_input_ids,\n",
    "                char_attention_mask=char_attention_mask,\n",
    "                device=device,\n",
    "                labels=targets,\n",
    "            )\n",
    "\n",
    "            predictions, true_predictions = model.get_predictions_from_logits(\n",
    "                logits=logits,\n",
    "                labels=targets,\n",
    "                corresponding_word=corresponding_word\n",
    "            )\n",
    "\n",
    "            # loss = loss_fn(predictions, true_predictions)\n",
    "            # loss = Variable(loss, requires_grad=True)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "            all_true.extend(true_predictions.tolist())\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    return np.mean(losses), (all_ids, all_true, all_predictions)\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    model,\n",
    "    num_epochs,\n",
    "    train_dataloader,\n",
    "    dev_dataloader,\n",
    "    loss_fn,\n",
    "    optimizer_config,\n",
    "    scheduler_config,\n",
    "    device,\n",
    "    metric_fn,\n",
    "    is_better_metric_fn,\n",
    "    num_epochs_before_finetune,\n",
    "    results_dir,\n",
    "):\n",
    "    history = defaultdict(list)\n",
    "    best_metric = None\n",
    "    best_model_state = None\n",
    "\n",
    "    optimizer = get_optimizer(model, optimizer_config, finetune=False)\n",
    "    scheduler = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        if epoch <= num_epochs_before_finetune:\n",
    "            print(\"Freeze transformer\")\n",
    "        else:\n",
    "            print(\"Finetune transformer\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        if epoch == num_epochs_before_finetune + 1:\n",
    "            model.unfreeze_transformer_layer()\n",
    "            optimizer = get_optimizer(model, optimizer_config, finetune=True)\n",
    "            scheduler = get_scheduler(\n",
    "                optimizer,\n",
    "                num_training_steps=len(train_dataloader) * num_epochs,\n",
    "                **scheduler_config,\n",
    "            )\n",
    "\n",
    "        train_loss, (train_ids, train_true, train_predict) = train_epoch(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            metric_fn,\n",
    "        )\n",
    "\n",
    "        train_metric = metric_fn(train_true, train_predict)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.5f}; Train Metric: {train_metric:.5f}\")\n",
    "\n",
    "        dev_loss, (dev_ids, dev_true, dev_predict) = validation_epoch(\n",
    "            model,\n",
    "            dev_dataloader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            metric_fn,\n",
    "        )\n",
    "\n",
    "        dev_metric = metric_fn(dev_true, dev_predict)\n",
    "\n",
    "        print(\n",
    "            f\"Validation Loss: {dev_loss:.5f}; \"\n",
    "            f\"Validation Metric: {dev_metric:.5f}\"\n",
    "        )\n",
    "\n",
    "        history[\"train_metric\"].append(train_metric)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"dev_metric\"].append(dev_metric)\n",
    "        history[\"dev_loss\"].append(dev_loss)\n",
    "\n",
    "        if best_metric is None or is_better_metric_fn(train_metric, best_metric):\n",
    "            best_metric = train_metric\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "            if results_dir is not None:\n",
    "                torch.save(\n",
    "                    best_model_state,\n",
    "                    os.path.join(results_dir, \"best_model.bin\"),\n",
    "                )\n",
    "\n",
    "                df_train_predictions = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": train_ids,\n",
    "                        \"true\": train_true,\n",
    "                        \"predict\": train_predict,\n",
    "                    }\n",
    "                )\n",
    "                df_train_predictions.to_csv(\n",
    "                    os.path.join(results_dir, \"best_model_train_predict.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "\n",
    "                df_dev_predictions = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": dev_ids,\n",
    "                        \"true\": dev_true,\n",
    "                        \"predict\": dev_predict,\n",
    "                    }\n",
    "                )\n",
    "                df_dev_predictions.to_csv(\n",
    "                    os.path.join(results_dir, \"best_model_dev_predict.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "\n",
    "    df_history = pd.DataFrame(history)\n",
    "    if results_dir is not None:\n",
    "        df_history.to_csv(os.path.join(results_dir, \"history.csv\"), index=False)\n",
    "\n",
    "        model.load_state_dict(torch.load(os.path.join(results_dir, \"best_model.bin\")))\n",
    "    else:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model_config\"][\"word_embedding_dim\"] = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocabulary.save_vocabulary(results_dir)\n",
    "word_vocabulary.save_vocabulary(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + \"/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model_config\"][\"word_pretrained_embedding\"] = embeddings\n",
    "config[\"model_config\"][\"word_freeze_embedding\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Freeze transformer\n",
      "----------\n",
      "Batch [1/229]; Loss: 2970.34375; Mean absolute error: 47.75000\n",
      "Batch [11/229]; Loss: 2958.27417; Mean absolute error: 96.62500\n",
      "Batch [21/229]; Loss: 1233.69031; Mean absolute error: 37.68750\n",
      "Batch [31/229]; Loss: 976.74646; Mean absolute error: 29.56250\n",
      "Batch [41/229]; Loss: 1364.63062; Mean absolute error: 39.37500\n",
      "Batch [51/229]; Loss: 972.86224; Mean absolute error: 24.50000\n",
      "Batch [61/229]; Loss: 867.24030; Mean absolute error: 26.25000\n",
      "Batch [71/229]; Loss: 392.81506; Mean absolute error: 11.12500\n",
      "Batch [81/229]; Loss: 499.95718; Mean absolute error: 17.06250\n",
      "Batch [91/229]; Loss: 265.18158; Mean absolute error: 9.31250\n",
      "Batch [101/229]; Loss: 352.47333; Mean absolute error: 9.81250\n",
      "Batch [111/229]; Loss: 221.82578; Mean absolute error: 10.06250\n",
      "Batch [121/229]; Loss: 336.50708; Mean absolute error: 8.93750\n",
      "Batch [131/229]; Loss: 225.04614; Mean absolute error: 8.93750\n",
      "Batch [141/229]; Loss: 391.61224; Mean absolute error: 23.81250\n",
      "Batch [151/229]; Loss: 400.08023; Mean absolute error: 10.87500\n",
      "Batch [161/229]; Loss: 106.74423; Mean absolute error: 6.62500\n"
     ]
    }
   ],
   "source": [
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "# model = CNNBiLSTMForTokenClassification(\n",
    "#     **config[\"model_config\"]\n",
    "# ).to(DEVICE)\n",
    "model = CNNBiLSTMWithCRFForTokenClassification(\n",
    "    **config[\"model_config\"]\n",
    ").to(DEVICE)\n",
    "loss_fn = get_loss_fn(config[\"training\"][\"loss\"], DEVICE)\n",
    "optimizer_config = config[\"training\"][\"optimizer\"]\n",
    "scheduler_config = config[\"training\"][\"scheduler\"]\n",
    "metric_fn, is_better_metric_fn = get_metric(config[\"training\"][\"metric\"])\n",
    "num_epochs_before_finetune = config[\"training\"][\"num_epochs_before_finetune\"]\n",
    "\n",
    "best_model, df_history = training_loop(\n",
    "    model,\n",
    "    num_epochs,\n",
    "    train_dataloader,\n",
    "    dev_dataloader,\n",
    "    loss_fn,\n",
    "    optimizer_config,\n",
    "    scheduler_config,\n",
    "    DEVICE,\n",
    "    metric_fn,\n",
    "    is_better_metric_fn,\n",
    "    num_epochs_before_finetune,\n",
    "    results_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    results_dir,\n",
    "    label_column,\n",
    "    file_format=\"csv\",\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            # print(batch)\n",
    "            ids = batch[\"id\"]\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            char_input_ids = batch[\"char_input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            char_attention_mask = batch[\"char_attention_mask\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            corresponding_word = batch[\"corresponding_word\"].to(device)\n",
    "\n",
    "            _, logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                char_input_ids=char_input_ids,\n",
    "                char_attention_mask=char_attention_mask,\n",
    "                device=device,\n",
    "                labels=None,\n",
    "            )\n",
    "\n",
    "            # print(f\"logits.shape: {logits.shape}\")\n",
    "            # print(f\"targets.shape: {targets.shape}\")\n",
    "\n",
    "            predictions, true_predictions = get_predictions_from_logits(\n",
    "                logits=logits,\n",
    "                labels=None,\n",
    "                corresponding_word=corresponding_word\n",
    "            )\n",
    "\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "            all_true.extend(true_predictions.tolist())\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    df_predictions = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": all_ids,\n",
    "            \"true\": all_true,\n",
    "            label_column: all_predictions,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if results_dir is not None:\n",
    "        if file_format == \"csv\":\n",
    "            df_predictions.to_csv(\n",
    "                os.path.join(results_dir, \"submission.csv\"),\n",
    "                index=False,\n",
    "            )\n",
    "        elif file_format == \"jsonl\":\n",
    "            df_predictions.to_json(\n",
    "                os.path.join(results_dir, \"submission.jsonl\"),\n",
    "                orient=\"records\",\n",
    "                lines=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown file format: {file_format}\")\n",
    "    else:\n",
    "        print(\"Missing results_dir, not saving predictions to file!\")\n",
    "\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = \"../runs/12-01-2024_19:43:36-SubtaskC-cnn_bilstm_for_token_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_vocabulary, word_vocabulary = None, None\n",
    "# char_max_len, word_max_len = None, config[\"data\"][\"max_len\"]\n",
    "# if dataset_type == DatasetType.TokenClassificationDataset:\n",
    "#     if dataset_type_settings is not None:\n",
    "#         if \"chars\" in dataset_type_settings:\n",
    "#             char_vocabulary = CharacterVocabulary()\n",
    "#             char_vocabulary.load_vocabulary(results_dir)\n",
    "\n",
    "#             char_max_len = dataset_type_settings[\"chars\"][\"max_len\"]\n",
    "\n",
    "#         if \"words\" in dataset_type_settings:\n",
    "#             word_vocabulary = WordVocabulary()\n",
    "#             word_vocabulary.load_vocabulary(results_dir)\n",
    "\n",
    "#             word_max_len = dataset_type_settings[\"words\"][\"max_len\"]\n",
    "#     else:\n",
    "#         word_vocabulary = WordVocabulary()\n",
    "#         word_vocabulary.load_vocabulary(results_dir)\n",
    "\n",
    "#         word_max_len = config[\"data\"][\"max_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{results_dir}/config.json\", \"r\") as config_file:\n",
    "#     config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = CNNBiLSTMForTokenClassification(**config[\"model_config\"]).to(DEVICE)\n",
    "# best_model.load_state_dict(\n",
    "#     torch.load(os.path.join(results_dir, \"best_model.bin\"))\n",
    "# )\n",
    "# best_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# test_dataloader = build_data_loader(\n",
    "#     df_test,\n",
    "#     tokenizer,\n",
    "#     max_len=word_max_len,\n",
    "#     batch_size=config[\"data\"][\"batch_size\"],\n",
    "#     label_column=config[\"data\"][\"label_column\"],\n",
    "#     has_targets=False,\n",
    "#     dataset_type=dataset_type,\n",
    "#     dataset_type_settings=dataset_type_settings,\n",
    "#     char_vocabulary=char_vocabulary,\n",
    "#     char_max_len=char_max_len,\n",
    "#     word_vocabulary=word_vocabulary,\n",
    "#     device=DEVICE,\n",
    "# )\n",
    "\n",
    "# make_predictions(\n",
    "#     best_model,\n",
    "#     test_dataloader,\n",
    "#     DEVICE,\n",
    "#     results_dir,\n",
    "#     label_column=config[\"data\"][\"label_column\"],\n",
    "#     file_format=\"csv\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_from_logits(logits, labels=None, corresponding_word=None):\n",
    "    # batch_size = logits.shape[0]\n",
    "\n",
    "    # logits: (batch_size, max_seq_len, out_size)\n",
    "    # labels: (batch_size, max_seq_len)\n",
    "    # corresponding_word: (batch_size, max_seq_len)\n",
    "\n",
    "    # print(f\"logits.shape: {logits.shape}\")\n",
    "    # print(f\"logits: {logits}\")\n",
    "\n",
    "    # preds: (batch_size, max_seq_len)\n",
    "    # preds = torch.argmax(logits, dim=-1)\n",
    "    preds = logits.clone()\n",
    "\n",
    "    # print(f\"preds.shape: {preds.shape}\")\n",
    "    # print(f\"preds: {preds}\")\n",
    "\n",
    "    if labels is not None:\n",
    "        # print(f\"labels.shape: {labels.shape}\")\n",
    "        # print(f\"labels: {labels}\")\n",
    "\n",
    "        # Keep only predictions where labels are not -100\n",
    "        # clean_preds = preds[labels != -100].reshape(batch_size, -1)\n",
    "        # clean_labels = labels[labels != -100].reshape(batch_size, -1)\n",
    "\n",
    "        # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "        # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "        # print(f\"clean_labels.shape: {clean_labels.shape}\")\n",
    "        # print(f\"clean_labels: {clean_labels}\")\n",
    "\n",
    "        # Get the index of the first machine text word\n",
    "        # predicted_positions = clean_preds.argmax(dim=-1)\n",
    "        # true_positions = clean_labels.argmax(dim=-1)\n",
    "\n",
    "        predicted_positions = []\n",
    "        true_positions = []\n",
    "        for p, l in zip(preds, labels):\n",
    "            mask = l != -100\n",
    "\n",
    "            # print(f\"mask: {mask}\")\n",
    "\n",
    "            clean_pred = p[mask]\n",
    "            clean_label = l[mask]\n",
    "\n",
    "            # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "            # print(f\"clean_pred: {clean_pred}\")\n",
    "            # print(f\"clean_label.shape: {clean_label.shape}\")\n",
    "            # print(f\"clean_label: {clean_label}\")\n",
    "\n",
    "            predicted_position = clean_pred.argmax(dim=-1)\n",
    "            true_position = clean_label.argmax(dim=-1)\n",
    "\n",
    "            # print(f\"predicted_position: {predicted_position}\")\n",
    "            # print(f\"true_position: {true_position}\")\n",
    "\n",
    "            predicted_positions.append(predicted_position.item())\n",
    "            true_positions.append(true_position.item())\n",
    "\n",
    "        # print(f\"predicted_positions.shape: {predicted_positions.shape}\")\n",
    "        # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "        # print(f\"true_positions.shape: {true_positions.shape}\")\n",
    "        # print(f\"true_positions: {true_positions}\")\n",
    "\n",
    "        # print(f\"predicted_positions type: {type(predicted_positions)}\")\n",
    "        # print(f\"true_positions type: {type(true_positions)}\")\n",
    "\n",
    "        return torch.Tensor(predicted_positions), torch.Tensor(true_positions)\n",
    "    elif corresponding_word is not None:\n",
    "        # print(f\"corresponding_word.shape: {corresponding_word.shape}\")\n",
    "        # print(f\"corresponding_word: {corresponding_word}\")\n",
    "\n",
    "        # Keep only predictions where corresponding_word are not -100\n",
    "        # clean_preds = preds[corresponding_word != -100].reshape(\n",
    "        #     batch_size, -1\n",
    "        # ).detach().cpu().numpy()\n",
    "        # clean_corresponding_word = corresponding_word[corresponding_word != -100].reshape(\n",
    "        #     batch_size, -1\n",
    "        # ).detach().cpu().numpy()\n",
    "\n",
    "        # print(f\"clean_preds.shape: {clean_preds.shape}\")\n",
    "        # print(f\"clean_preds: {clean_preds}\")\n",
    "\n",
    "        # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "        # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "        predicted_positions = []\n",
    "        for p, w in zip(preds, corresponding_word):\n",
    "            mask = w != -100\n",
    "\n",
    "            # print(f\"mask.shape: {mask.shape}\")\n",
    "            # print(f\"mask: {mask}\")\n",
    "\n",
    "            clean_pred = p[mask]\n",
    "            clean_corresponding_word = w[mask]\n",
    "\n",
    "            # print(f\"clean_pred.shape: {clean_pred.shape}\")\n",
    "            # print(f\"clean_pred: {clean_pred}\")\n",
    "            # print(f\"clean_corresponding_word.shape: {clean_corresponding_word.shape}\")\n",
    "            # print(f\"clean_corresponding_word: {clean_corresponding_word}\")\n",
    "\n",
    "            # Get the index of the first machine text word\n",
    "            index = torch.where(clean_pred == 1)[0]\n",
    "            # print(f\"index: {index}\")\n",
    "            # print(f\"index.size: {index.size}\")\n",
    "            value = index[0] if len(index) > 0 else len(clean_pred) - 1\n",
    "            position = clean_corresponding_word[value]\n",
    "\n",
    "            # print(f\"index: {index}\")\n",
    "            # print(f\"value: {value}\")\n",
    "            # print(f\"position: {position}\")\n",
    "\n",
    "            predicted_positions.append(position.item())\n",
    "        #     # pred = pred.detach().cpu().numpy()\n",
    "\n",
    "        #     index = np.where(pred == 1)[0]\n",
    "        #     value = index[0] if index.size else len(pred) - 1\n",
    "        #     position = clean_corresponding_word[idx][value]\n",
    "\n",
    "        #     predicted_positions.append(position.item())\n",
    "\n",
    "        # print(f\"predicted_positions: {predicted_positions}\")\n",
    "\n",
    "        return torch.Tensor(predicted_positions), torch.Tensor([-1] * len(predicted_positions))\n",
    "    else:\n",
    "        raise ValueError(\"Either labels or corresponding_word must be provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1113/1113 [1:01:47<00:00,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = make_predictions(\n",
    "    best_model,\n",
    "    test_dataloader,\n",
    "    DEVICE,\n",
    "    results_dir,\n",
    "    config[\"data\"][\"label_column\"],\n",
    "    file_format=\"csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../runs/17-01-2024_08:49:45-SubtaskC-cnn_bilstm_with_crf_for_token_classification'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation\n",
      "MAE: 8.34653\n",
      "--------------------\n",
      "Results on test\n",
      "MAE: 157.67752\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "!python ../scores_and_plots.py --results-dir \"../runs/15-01-2024_19:43:53-SubtaskC-cnn_bilstm_with_crf_for_token_classification\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
